% !TEX root = main.tex

\chapter{Grammars}\label{C-grammars}


\startchapter{Both natural languages, }such as English and the
artificial languages used for programming have a structure
known as grammar or syntax.  In order to form legal sentences
or programs, the parts of the language must be fit together
according to certain rules.  For natural languages, the
rules are somewhat informal (although high-school English
teachers might have us believe differently).  For programming
languages, the rules are absolute, and programs that violate
the rules will be rejected by a compiler.

In this chapter, we will study formal grammars and languages
defined by them.  The languages we look at will, for the most part,
be ``toy'' languages, compared to natural languages or even
to programming languages, but the ideas and techniques are basic
to any study of language.  In fact, many of the ideas arose
almost simultaneously in the 1950s in the work of linguists who were
studying natural language and programmers who were looking for
ways to specify the syntax of programming languages.

The grammars in this chapter are \nw[generative grammar]{generative grammars}.
A generative grammar is a set of rules that can be used to generate
all the legal strings in a language.  We will also consider the closely
related idea of \nw{parsing}.  To parse a string means to determine
how that string can be generated according to the rules.

This chapter is a continuation of the preceding chapter.  
Like a regular expression, a grammar is a way to specify a possibly
infinite language with a finite amount of information.  In fact,
we will see that every regular language can be specified
by a certain simple type of grammar.  We will also see that some languages
that can be specified by grammars are not regular.

\section{Context-free Grammars}\label{S-grammars-1}

In its most general form, a grammar is a set of \nw[rewriting rule]{rewriting
rules}.  A rewriting rule specifies that a certain string of symbols can
be substituted for all or part of another string.  If $w$ and $u$ are
strings, then $w\PRODUCES u$ is a rewriting rule that specifies that
the string $w$ can be replaced by the string $u$.  The symbol ``$\PRODUCES$''
is read ``can be rewritten as.''  Rewriting rules are also called
\nw[production rule]{production rules} or \nw{productions}, and
``$\PRODUCES$'' can also be read as ``produces.''  For example,
if we consider strings over the alphabet $\{a,b,c\}$, then
the production rule $aba\PRODUCES cc$ can be applied to the
string $abbabac$ to give the string $abbccc$.  The substring $aba$
in the string $abbabac$ has been replaced with $cc$.

In a \nw{context-free grammar}, every rewriting rule has the
form $A\PRODUCES w$, where $A$ is single symbol and $w$ is a string
of zero or more symbols.  (The grammar is ``context-free'' in the
sense that $w$ can be substituted for $A$ wherever $A$ occurs in a string,
regardless of the surrounding context in which $A$ occurs.)
The symbols that occur on the left-hand
sides of production rules in a context-free grammar
are called \nw[non-terminal symbol]{non-terminal symbols}.
By convention, the non-terminal symbols are usually  uppercase letters.
The strings on the right-hand sides of the production rules can
include non-terminal symbols as well as other symbols, which are
called \nw[terminal symbol]{terminal symbols}.  By convention, the
terminal symbols are usually lowercase letters.  Here are some
typical production rules that might occur in context-free grammars:
\begin{align*}
   A&\PRODUCES aAbB\\
   S&\PRODUCES SS\\
   C&\PRODUCES Acc\\
   B&\PRODUCES b\\
   A&\PRODUCES\EMPTYSTRING
\end{align*}
In the last rule in this list, $\EMPTYSTRING$ represents the empty string,
as usual.  For example, this rule could be applied to the string
$aBaAcA$ to produce the string $aBacA$.  The first occurrence of
the symbol $A$ in $aBaAcA$ has been replaced by the empty string---which
is just another way of saying that the symbol has been dropped from the string.

In every context-free grammar, one of the non-terminal symbols is
designated as the \nw{start symbol} of the grammar.  The start symbol
is often, though not always, denoted by~$S$.  When the grammar
is used to generate strings in a language, the idea is to start
with a string consisting of nothing but the start symbol.  Then
a sequence of production rules is applied.  Each application of
a production rule to the string transforms the string to a new
string.  If and when this process produces a string that consists
purely of terminal symbols, the process ends.  That string of
terminal symbols is one of the strings in the language generated
by the grammar.  In fact, the language consists precisely of
all strings of terminal symbols that can be produced in this way.

As a simple example, consider a grammar that has three production
rules: $S\PRODUCES aS$, $S\PRODUCES bS$, and $S\PRODUCES b$.
In this example, $S$ is the only non-terminal symbol, and
the terminal symbols are $a$ and $b$.  Starting from the
string $S$, we can apply any of the three rules of the grammar
to produce either $aS$, $bS$, or $b$.  Since the string $b$ contains
no non-terminals, we see that $b$ is one of the strings in the language
generated by this grammar.  The strings $aS$ and $bS$ are not in
that language, since they contain the non-terminal symbol $S$,
but we can continue to apply production rules to these strings.
From $aS$, for example, we can obtain $aaS$, $abS$, or $ab$.
From $abS$, we go on to obtain $abaS$, $abbS$, or $abb$.
The strings $ab$ and $abb$ are in the language generated by
the grammar.  It's not hard to see that any string of $a$'s and
$b$'s that ends with a $b$ can be generated by this grammar,
and that these are the only strings that can be generated.
That is, the language generated by this grammar is the regular
language specified by the regular expression $(a\REOR b)^{*}b$.

It's time to give some formal definitions of the concepts which
we have been discussing.

\begin{definition}
A \nw{context-free grammar} is a 4-tuple $(V,\Sigma,P,S)$,
where:

\Item{1.\ }$V$ is a finite set of symbols.  The elements of $V$
are the non-terminal symbols of the grammar.

\Item{2.\ }$\Sigma$ is a finite set of symbols such that $V\cap\Sigma=\emptyset$.
The elements of $\Sigma$ are the terminal symbols of the grammar.

\Item{3.\ }$P$ is a set of production rules.  Each rule is of the
form $A\PRODUCES w$ where $A$ is one of the symbols in $V$ and
$w$ is a string in the language $(V\cup\Sigma)^*$.

\Item{4.\ }$S\in V$.  $S$ is the start symbol of the grammar.

\end{definition}

Even though this is the formal definition, grammars are often
specified informally simply by listing the set of production rules.
When this is done it is assumed, unless otherwise specified,
that the non-terminal symbols are just the symbols that occur
on the left-hand sides of production rules of the grammar.
The terminal symbols are all the other symbols that occur on
the right-hand sides of production rules.  The start symbol is the
symbol that occurs on the left-hand side of the first production
rule in the list.  Thus, the list of production rules
\begin{align*}
   T&\PRODUCES TT\\
   T&\PRODUCES A\\
   A&\PRODUCES aAa\\
   A&\PRODUCES bB\\
   B&\PRODUCES bB\\
   B&\PRODUCES \EMPTYSTRING 
\end{align*}
specifies a grammar $G=(V,\Sigma,P,T)$ where $V$ is $\{T,A,B\}$,
$\Sigma$ is $\{a,b\}$, and $T$ is the start symbol.  $P$,~of course, is a
set containing the six production rules in the list.


Let $G=(V,\Sigma,P,S)$ be a context-free grammar.
Suppose that $x$ and $y$ are strings in the language $(V\cup\Sigma)^*$.
The notation $x\YIELDS_G y$ is used to express the fact
that $y$ can be obtained from $x$ by applying one of the production
rules in $P$.  To be more exact, we say that $x\YIELDS_G y$
if and only if there is a production rule $A\PRODUCES w$ in the grammar
and two strings $u$ and $v$ in the language $(V\cup\Sigma)^*$
such that $x=uAv$ and $y=uwv$. The fact
that $x=uAv$ is just a way of saying that $A$ occurs somewhere in
$x$.  When the production rule $A\PRODUCES w$ is applied to
substitute $w$ for $A$ in $uAv$, the result is $uwv$, which is $y$.
Note that either $u$ or $v$ or both can be the empty string.

If a string $y$ can be obtained from a string $x$ by
applying a sequence of zero or more production rules, we
write $x\YIELDS_G^* y$.  In most cases, the ``$G$'' in
the notations $\YIELDS_G$ and $\YIELDS_G^*$ will be omitted,
assuming that the grammar in question is understood.
Note that $\YIELDS$ is a relation on the set $(V\cup\Sigma)^*$.
The relation $\YIELDSTAR$ is the reflexive, transitive closure of that relation.
(This explains the use of ``$*$'', which is usually used to
denote the transitive, but not necessarily reflexive, closure of a relation. 
In this case, $\YIELDSTAR$ is reflexive as well as transitive since
$x\;\YIELDSTAR x$ is true for any string~$x$.)
For example, using the grammar that is defined by the above
list of production rules, we have
\begin{align*}
 aTB&\YIELDS aTTB\\
    &\YIELDS aTAB\\
    &\YIELDS aTAbB\\
    &\YIELDS aTbBbB\\
    &\YIELDS aTbbB
\end{align*}
From this, it follows that $aTB\;\YIELDSTAR aTbbB$.  The relation $\YIELDS$
is read ``yields'' or ``produces'' while $\YIELDSTAR$ can be
read ``yields in zero or more steps'' or ``produces in zero or more
steps.''  The following theorem states some simple facts about
the relations $\YIELDS$ and $\YIELDSTAR$:

\begin{theorem}\label{T-yields}
Let $G$ be the context-free grammar $(V,\Sigma,P,S)$.
Then:
\Item{1. }If $x$ and $y$ are strings in $(V\cup\Sigma)^*$ such that $x\YIELDS y$, 
then $x\;\YIELDSTAR y$.
\Item{2. }If $x$, $y$, and $z$ are strings in $(V\cup\Sigma)^*$ such that $x\;\YIELDSTAR y$
and $y\;\YIELDSTAR z$, then $x\;\YIELDSTAR z$.
\Item{3. }If $x$ and $y$ are strings in $(V\cup\Sigma)^*$ such that $x\YIELDS y$, 
and if $s$ and $t$ are any strings in $(V\cup\Sigma)^*$, then $sxt\YIELDS syt$.
\Item{4. }If $x$ and $y$ are strings in $(V\cup\Sigma)^*$ such that $x\;\YIELDSTAR y$, 
and if $s$ and $t$ are any strings in $(V\cup\Sigma)^*$, then $sxt\;\YIELDSTAR syt$.
\end{theorem}
\begin{proof}
Parts 1 and 2 follow from the fact that $\YIELDSTAR$ is the transitive
closure of $\YIELDS$.  Part 4 follows easily from Part 3.  (I leave this
as an exercise.)  To prove Part 3, suppose that $x$, $y$, $s$, and $t$
are strings such that $x\YIELDS y$.  By definition, this means that
there exist strings $u$ and $v$ and a production rule $A\PRODUCES w$
such that $x=uAv$ and $y=uwv$.  But then we also have
$sxt=suAvt$ and $syt=suwvt$.  These two equations, along with
the existence of the production rule $A\PRODUCES w$ show, by definition,
that $sxt\YIELDS syt$.
\end{proof}

We can use $\YIELDSTAR$ to give a formal definition
of the language generated by a context-free grammar:

\begin{definition}
Suppose that  $G=(V,\Sigma,P,S)$ is a context-free grammar.
Then the language generated by $G$ is the language $L(G)$ over
the alphabet $\Sigma$ defined by
\[L(G)=\{w\in \Sigma^*\st S\YIELDS_G^* w\}\]
That is, $L(G)$ contains any string of terminal symbols that can be 
obtained by starting with the string consisting of the start symbol, $S$, 
and applying a sequence of production rules.  

A language $L$ is said to be a \nw{context-free language} if
there is a context-free grammar $G$ such that $L(G)$ is $L$.
Note that there might be many different context-free grammars
that generate the same context-free language.  Two context-free
grammars that generate the same language are said to be
\nw[equivalent grammars]{equivalent}.
\end{definition}

Suppose $G$ is a context-free grammar with start symbol $S$
and suppose $w\in L(G)$.  By definition, this means that
there is a sequence of one or more applications of production rules
which produces the string $w$ from $S$.  This sequence has the
form $S\YIELDS x_1\YIELDS x_2\YIELDS\cdots\YIELDS w$.  Such a sequence
is called a \nw{derivation} of $w$ (in the grammar $G$).  Note
that $w$ might have more than one derivation.  That is, it might
be possible to produce $w$ in several different ways.

Consider the language $L=\{a^nb^n\st n\in\N\}$.  We already know
that $L$ is not a regular language.  However, it is a context-free
language.  That is, there is a context-free grammar such that
$L$ is the language generated by $G$.  This gives us our first
theorem about grammars:

\begin{theorem}
Let $L$ be the language $L=\{a^nb^n\st n\in\N\}$.  Let $G$ be
the context-free grammar $(V,\Sigma,P,S)$ where $V=\{S\}$,
$\Sigma=\{a,b\}$ and $P$ consists of the productions
\begin{align*}
     S&\PRODUCES aSb\\
     S&\PRODUCES \EMPTYSTRING 
\end{align*}
Then $L=L(G)$, so that $L$ is a context-free language.  In particular,
there exist context-free languages which are not regular.
\end{theorem}
\begin{proof}
To show that $L=L(G)$, we must show both that $L\SUB L(G)$
and that $L(G)\SUB L$.  To show that $L\SUB L(G)$, let $w$
be an arbitrary element of $L$.  By definition of $L$,
$w=a^nb^n$ for some $n\in\N$.  We show that $w\in L(G)$ by
induction on $n$.  In the case where $n=0$, we have $w=\EMPTYSTRING$.
Now, $\EMPTYSTRING\in L(G)$ since $\EMPTYSTRING$ can be produced
from the start symbol $S$ by an application of the rule $S\PRODUCES\EMPTYSTRING$,
so our claim is true for $n=0$.
Now, suppose that $k\in\N$ and that we already know that $a^kb^k\in L(G)$.
We must show that $a^{k+1}b^{k+1}\in L(G)$.  Since 
$S\;\YIELDSTAR a^kb^k$, we also have, by Theorem \ref{T-yields}, that
$aSb\;\YIELDSTAR aa^kb^kb$.
That is, $aSb\;\YIELDSTAR a^{k+1}b^{k+1}$.  Combining this with the
production rule $S\PRODUCES aSb$, we see that $S\;\YIELDSTAR a^{k+1}b^{k+1}$.
This means that $a^{k+1}b^{k+1}\in L(G)$, as we wanted to show.
This completes the proof that $L\SUB L(G)$.

To show that $L(G)\SUB L$, suppose that $w\in L(G)$.  That is,
$S\;\YIELDSTAR w$.  We must show that $w=a^nb^n$ for some $n$.
Since $S\;\YIELDSTAR w$, there is a derivation
$S\YIELDS x_0\YIELDS x_1\YIELDS\cdots\YIELDS x_n$, where $w=x_n$.
We first prove by induction on $n$ that in any derivation
$S\YIELDS x_0\YIELDS x_1\YIELDS\cdots\YIELDS x_n$,
we must have either $x_n=a^nb^n$ or $x_n=a^{n+1}Sb^{n+1}$.
Consider the case $n=0$.  Suppose $S\YIELDS x_0$.
Then, we must have that $S\PRODUCES x_0$ is a rule in the grammar,
so $x_0$ must be either $\EMPTYSTRING$ or $aSb$. Since $\EMPTYSTRING=a^0b^0$
and $aSb=a^{0+1}Sb^{0+1}$, $x_0$ is of the required form. 
Next, consider the inductive case.  Suppose that $k>1$ and we already 
know that in any
derivation $S\YIELDS x_0\YIELDS x_1\YIELDS\cdots\YIELDS x_k$,
we must have $x_k=a^kb^k$ or $x=a^{k+1}Sb^{k+1}$.  Suppose that
$S\YIELDS x_0\YIELDS x_1\YIELDS\cdots\YIELDS x_k\YIELDS x_{k+1}$.
We know by induction that $x_k=a^kb^k$ or $x=a^{k+1}Sb^{k+1}$,
but since $x_k\YIELDS x_{k+1}$ and $a^kb^k$ contains no non-terminal
symbols, we must have $x_k=a^{k+1}Sb^{k+1}$.  Since $x_{k+1}$
is obtained by applying one of the production rules $S\PRODUCES\EMPTYSTRING$
or $S\PRODUCES aSb$ to~$x_k$, $x_{k+1}$ is either $a^{k+1}\EMPTYSTRING b^{k+1}$
or $a^{k+1}aSbb^{k+1}$.  That is, $x_{k+1}$ is either $a^{k+1}b^{k+1}$
or $a^{k+2}Sb^{k+2}$, as we wanted to show.  This completes the induction.
Turning back to $w$, we see that $w$ must be of the form $a^nb^n$ or
of the form $a^nSb^n$.  But since $w\in L(G)$, it can contain no
non-terminal symbols, so $w$ must be of the form $a^nb^n$, as we wanted to show.
This completes the proof that $L(G)\SUB L$.
\end{proof}
I have given a very formal and detailed proof of this theorem, to show how it
can be done and to show how induction plays a role in many proofs about
grammars.  However, a more informal proof of the theorem would probably
be acceptable and might even be more convincing.  To show that
$L\SUB L(G)$, we could just note that the derivation
$S\YIELDS aSb\YIELDS a^2Sb^2\YIELDS\cdots\YIELDS a^nSb^n\YIELDS a^nb^n$
demonstrates that $a^nb^n\in L$.  On the other hand, it is clear that every
derivation for this grammar must be of this form, so every string in $L(G)$
is of the form $a^nb^n$.

For another example, consider the language $\{a^nb^m\st n\ge m\ge0\}$.
Let's try to design a grammar that generates this language.
This is similar to the previous example, but now we want to include strings that
contain more $a$'s than $b$'s.  The production rule $S\PRODUCES aSb$
always produces the same number of $a$'s and $b$'s.  Can we modify
this idea to produce more $a$'s than $b$'s?

One approach would be to produce a string containing just as many
$a$'s as $b$'s, and then to add some extra $a$'s.  A rule that can
generate any number of $a$'s is $A\PRODUCES aA$. After
applying the rule $S\PRODUCES aSb$ for a while, we want to move
to a new state in which we apply the rule $A\PRODUCES aA$.  We can
get to the new state by applying a rule $S\PRODUCES A$
that changes the $S$ into an $A$.  We still need a way to 
finish the process, which means getting rid of all non-terminal
symbols in the string.  For this, we can use the rule $A\PRODUCES\EMPTYSTRING$.
Putting these rules together, we get the grammar
\begin{align*}
      S&\PRODUCES aSb\\
      S&\PRODUCES A\\
      A&\PRODUCES aA\\
      A&\PRODUCES \EMPTYSTRING
\end{align*}
This grammar does indeed generate the language $\{a^nb^m\st n\ge m\ge 0\}$.
With slight variations on this grammar, we can produce other related
languages.  For example, if we replace the rule $A\PRODUCES \EMPTYSTRING$
with $A\PRODUCES a$, we get the language $\{a^nb^m\st n> m\ge 0\}$.

There are other ways to generate the language $\{a^nb^m\st n\ge m\ge 0\}$.
For example, the extra non-terminal symbol, $A$, is not really necessary,
if we allow $S$ to sometimes produce a single $a$ without a $b$.  This
leads to the grammar
\begin{align*}
      S&\PRODUCES aSb\\
      S&\PRODUCES aS\\
      S&\PRODUCES \EMPTYSTRING
\end{align*}
(But note that the rule $S\PRODUCES Sa$ would not work in place
of $S\PRODUCES aS$, since it would allow the production of
strings in which an $a$ can follow a $b$, and there are no
such strings in the language $\{a^nb^m\st n\ge m\ge 0\}$.)
And here are two more grammars that generate this language:
\begin{align*}
      S&\PRODUCES AB             &\qquad S&\PRODUCES ASb\\
      A&\PRODUCES aA             &\qquad A&\PRODUCES aA\\
      B&\PRODUCES aBb            &\qquad S&\PRODUCES\EMPTYSTRING\\
      A&\PRODUCES \EMPTYSTRING   &\qquad A&\PRODUCES\EMPTYSTRING\\
      B&\PRODUCES \EMPTYSTRING   &\qquad  &
\end{align*}

\medbreak

Consider another variation on the language $\{a^nb^n\st n\in\N\}$,
in which the $a$'s and $b$'s can occur in any order, but the number
of $a$'s is still equal to the number of $b$'s.  This language
can be defined as
$L=\{w\in\{a,b\}^*\st n_a(w) = n_b(w)\}$.  This language includes
strings such as $abbaab$, $baab$, and $bbbaaa$.

Let's start with the grammar containing the rules $S\PRODUCES aSb$
and $S\PRODUCES\EMPTYSTRING$.  We can try adding the rule
$S\PRODUCES bSa$.  Every string that can be generated using these
three rules is in the language $L$.  However, not every string
in $L$ can be generated.  A derivation that starts with $S\YIELDS aSb$
can only produce strings that begin with $a$ and end with $b$.
A derivation that starts with $S\YIELDS bSa$ can only generate strings
that begin with $b$ and end with $a$.  There is no way to generate
the strings $baab$ or $abbbabaaba$, which are in the language $L$.
But we shall see that any string in $L$ that begins and
ends with the same letter can be written in the form $xy$ where
$x$ and $y$ are shorter strings in $L$.  To produce strings of
this form, we need one more rule, $S\PRODUCES SS$.  The complete set of
production rules for the language $L$ is
\begin{align*}
    S&\PRODUCES aSb\\
    S&\PRODUCES bSa\\
    S&\PRODUCES SS\\
    S&\PRODUCES \EMPTYSTRING
\end{align*}
It's easy to see that every string that can be generated using these
rules is in $L$, since each rule introduces the same number of
$a$'s as $b$'s.  But we also need to check that every string
$w$ in $L$ can be generated by these rules.  This can be done
by induction on the length of $w$, using the second form
of the principle of mathematical induction.  In the base case,
$|w|=0$ and $w=\EMPTYSTRING$.  In this case, $w\in L$ since
$S\YIELDS\EMPTYSTRING$ in one step.
Suppose $|w|=k$, where $k>0$, and suppose that
we already know that for any $x\in L$ with $|x|<k$, $S\;\YIELDSTAR x$.
To finish the induction we must show, based on this induction 
hypothesis, that $S\;\YIELDSTAR w$.

Suppose that the first and last characters of $w$ are
different.  Then $w$ is either of the form $axb$ or of the form $bxa$,
for some string $x$.  Let's assume that $w$ is of the form $axb$.
(The case where $w$ is of the form $bxa$ is handled in a similar way.)
Since $w$ has the same number of $a$'s and $b$'s
and since $x$ has one fewer $a$ than $w$ and one fewer $b$ than $w$,
$x$ must also have the same number of $a$'s as $b$'s.  That is $x\in L$.
But $|x|=|w|-2<k$, so by the induction hypothesis, $x\in L(G)$.
So we have $S\;\YIELDSTAR x$.  By Theorem~\ref{T-yields}, we
get then $aSb\;\YIELDSTAR axb$.  Combining this with the fact
that $S\YIELDS aSb$, we get that $S\;\YIELDSTAR axb$, that is,
$S\;\YIELDSTAR w$.  This proves that $w\in L(G)$.

Finally, suppose that the first and last characters of $w$ are
the same.  Let's say that $w$ begins and ends with $a$.  (The case
where $w$ begins and ends with $b$ is handled in a similar way.)
I claim that $w$ can be written in the form $xy$ where $x\in L(G)$
and $y\in L(G)$ and neither $x$ nor $y$ is the empty string.
This will finish the induction, since we will then have by
the induction hypothesis that $S\;\YIELDSTAR x$
and $S\;\YIELDSTAR y$, and we can derive $xy$ from $S$ by first
applying the rule $S\PRODUCES SS$ and then using the first
$S$ on the right-hand side to derive $x$ and the second to derive $y$.

It only remains to figure out how to divide $w$ into two strings
$x$ and $y$ which are both in $L(G)$.  The technique that is used
is one that is more generally useful.  Suppose that $w=c_1c_2\cdots c_k$,
where each $c_i$ is either $a$ or $b$.  Consider the sequence of
integers $r_1$, $r_2$, \dots, $r_k$ where for each $i = 1, 2, \dots, k$,
$r_i$ is the number of $a$'s in $c_1c_2\cdots c_i$ minus the
number of $b$'s in $c_1c_2\cdots c_i$.  Since $c_1=a$, $r_1=1$.
Since $w\in L$, $r_k=0$.  And since $c_k=a$, we must have $r_{k-1}=
r_k-1 = -1$.  Furthermore the difference between $r_{i+1}$
and $r_i$ is either $1$ or~$-1$, for $i=1,2,\dots,k-1$.

Since $r_1=1$ and $r_{k-1}=-1$ and the value of $r_i$ goes up or down
by 1 when $i$ increases by 1, $r_i$ must be zero for some $i$
between 1 and $k-1$.  That is, $r_i$ cannot get from 1 to~$-1$ unless
it passes through zero. Let $i$ be a number between 1 and $k-1$ such
that $r_i=0$.  Let $x=c_1c_2\cdots c_i$ and let $y=c_{i+1}c_{i+2}\cdots c_k$.
Note that $xy=w$.  The fact that $r_i=0$ means that 
the string $c_1c_2\cdots c_i$ has the same number of $a$'s and
$b$'s, so $x\in L(G)$.  It follows automatically that $y\in L(G)$
also.  Since $i$ is strictly between 1 and $k-1$, neither $x$ nor
$y$ is the empty string.  This is all that we needed to show
to finish the proof that $L=L(G)$.

The basic idea of this proof is that if $w$ contains the same 
number of $a$'s as $b$'s, then an $a$ at the beginning
of $w$ must have a ``matching'' $b$ somewhere in $w$.  This
$b$ matches the $a$ in the sense that the corresponding $r_i$ is
zero, and the $b$ marks the end of a string $x$ which contains
the same number of $a$'s as $b$'s.  For example, in the
string $aababbabba$, the $a$ at the beginning of the string
is matched by the third $b$, since $aababb$ is the shortest
prefix of $aababbabba$ that has an equal number of $a$'s
and $b$'s.

Closely related to this idea of matching $a$'s and $b$'s is
the idea of \nw{balanced parentheses}.  Consider a string
made up of parentheses, such as \texttt{(()(()))(())}.
The parentheses in this sample string are balanced because
each left parenthesis has a matching right parenthesis,
and the matching pairs are properly nested.  A careful definition
uses the sort of integer sequence introduced in the above
proof.  Let $w$ be a string of parentheses.  Write
$w=c_1c_2\cdots c_n$, where each $c_i$ is either \texttt{(}
or \texttt{)}.  Define a sequence of integers $r_1$, $r_2$, \dots,~$r_n$,
where $r_i$ is the number of left parentheses in $c_1c_2\cdots c_i$
minus the number of right parentheses.  We say that the parentheses
in $w$ are balanced if $r_n=0$ and $r_i\ge0$ for all $i=1,2,\dots,n$.
The fact that $r_n=0$ says that $w$ contains the same number of
left parentheses as right parentheses.  The fact the $r_i\ge0$
means that the nesting of pairs of parentheses is correct:
You can't have a right parenthesis unless it is balanced by a left
parenthesis in the preceding part of the string.  The language
that consists of all balanced strings of parentheses is context-free.  
It is generated by the grammar
\begin{align*}
   S&\PRODUCES (\,S\,)\\
   S&\PRODUCES SS\\
   S&\PRODUCES \EMPTYSTRING
\end{align*}
The proof is similar to the preceding proof about strings of
$a$'s and $b$'s.  (It might seem that I've made an awfully big
fuss about matching and balancing.  The reason is that this
is one of the few things that we can do with context-free languages
that we can't do with regular languages.)

\medbreak

Before leaving this section, we should look at a few more
general results.  Since we know that most operations on regular
languages produce languages that are also regular, we can
ask whether a similar result holds for context-free languages.
We will see later that the intersection of two context-free
languages is not necessarily context-free.  Also, the
complement of a context-free language is not necessarily
context-free.  However, some other operations on context-free
languages do produce context-free languages.

\begin{theorem}\label{T-CFG-closures}
Suppose that $L$ and $M$ are context-free languages.
Then the languages $L\cup M$, $LM$, and $L^*$ are also
context-free.
\end{theorem}
\begin{proof}
I will prove only the first claim of the theorem, that $L\cup M$ is
context-free.  In the exercises for this section, you are
asked to construct grammars for $LM$ and $L^*$ (without giving
formal proofs that your answers are correct).

Let $G=(V,\Sigma,P,S)$ and $H=(W,\Gamma,Q,T)$ be context-free grammars
such that $L=L(G)$ and $M=L(H)$.  We can assume that $W\cap V=\emptyset$,
since otherwise we could simply rename the non-terminal symbols in $W$.
The idea of the proof is that to generate a string in $L\cup M$,
we first decide whether we want a string in $L$ or a string in $M$.
Once that decision is made, to make a string in $L$, we use production
rules from $G$, while to make a string in $M$, we use rules from $H$.
We have to design a grammar, $K$, to represent this process.

Let $R$ be a symbol that is not in any of the alphabets $V$, $W$, $\Sigma$,
or $\Gamma$.  $R$ will be the start symbol of $K$.  The production rules
for $K$ consist of all the production rules from $G$ and $H$ together
with two new rules:
\begin{align*}
   R\PRODUCES S\\
   R\PRODUCES T
\end{align*}
Formally, $K$ is defined to be the grammar
\[ (V\cup W\cup\{R\},
       P\cup Q\cup \{R\PRODUCES S, R\PRODUCES T\},
       \Sigma\cup\Gamma, R) \]
Suppose that $w\in L$.  That is $w\in L(G)$, so there is
a derivation $S\YIELDS_G^*w$.
Since every rule from $G$ is also a rule in $K$, if follows that
$S\YIELDS_K^* w$.  Combining this with the fact that $R\YIELDS_K S$, we have
that $R\YIELDS_K^* w$, and $w\in L(K)$.  This shows that $L\SUB L(K)$.
In an exactly similar way, we can show that $M\SUB L(K)$.
Thus, $L\cup M\SUB L(K)$.

It remains to show that $L(K)\SUB L\cup M$.  Suppose $w\in L(K)$.
Then there is a derivation $R\YIELDS_K^*w$.  This derivation must
begin with an application of
one of the rules $R\PRODUCES S$ or $R\PRODUCES T$, since these are the
only rules in which $R$ appears.  If the first rule applied in the
derivation is $R\PRODUCES S$, then the remainder of the derivation
shows that $S\YIELDS_K^*w$.  Starting from $S$, the only rules
that can be applied are rules from $G$, so in fact we have
$S\YIELDS_G^*w$.  This shows that $w\in L$.  Similarly, if
the first rule applied in the derivation $R\YIELDS_K^*w$ is 
$R\PRODUCES T$, then $w\in M$.  In any case, $w\in L\cup M$.
This proves that $L(K)\SUB L\cup M$.
\end{proof}

Finally, we should clarify the relationship between context-free
languages and regular languages.  We have already seen that
there are context-free languages which are not regular.
On the other hand, it turns out that every regular language
is context-free.  That is, given any regular language, there
is a context-free grammar that generates that language.  This
means that any syntax that can be expressed by a regular expression,
by a DFA, or by an NFA could also be expressed by a context-free
grammar.  In fact, we only need a certain restricted type of
context-free grammar to duplicate the power of regular expressions.

\begin{definition}
A \nw{right-regular grammar}\index{regular grammar} is a context-free 
grammar in which the right-hand side of every production
rule has one of the following forms:  the empty string;
a string consisting of a single non-terminal symbol; or
a string consisting of a single terminal symbol followed
by a single non-terminal symbol.
\end{definition}

Examples of the types of production rule that are allowed in
a right-regular grammar are $A\PRODUCES\EMPTYSTRING$,
$B\PRODUCES C$, and $D\PRODUCES aE$.  The idea of the
proof is that given a right-regular grammar, we can build
a corresponding $NFA$ and \textit{vice-versa}.  The
states of the $NFA$ correspond to the non-terminal symbols
of the grammar.  The start symbol of the grammar corresponds
to the starting state of the NFA.
A production rule of the form $A\PRODUCES bC$
corresponds to a transition in the NFA from state $A$ to state
$C$ while reading the symbol $b$.  A production rule of
the form $A\PRODUCES B$ corresponds to an $\EMPTYSTRING$-transition
from state $A$ to state $B$ in the NFA.  And a production
rule of the form $A\PRODUCES\EMPTYSTRING$ exists in the grammar
if and only if $A$ is a final state in the NFA.  With this
correspondence, a derivation of a string $w$ in the grammar
corresponds to an execution path through the NFA as it 
accepts the string $w$.  I won't give a complete proof
here.  You are welcome to work through the details if you want.
But the important fact is:

\begin{theorem}\label{T-reggram}
A language $L$ is regular if and only if there is a right-regular
grammar $G$ such that $L=L(G)$.  In particular, every regular
language is context-free.
\end{theorem}


\begin{exercises}

\problem Show that Part 4 of Theorem \ref{T-yields} follows from Part 3.

\problem Give a careful proof that the language $\{a^nb^m\st n\ge m\ge 0\}$
is generated by the context-free grammar
\begin{align*}
      S&\PRODUCES aSb\\
      S&\PRODUCES A\\
      A&\PRODUCES aA\\
      A&\PRODUCES \EMPTYSTRING
\end{align*}

\problem Identify the language generated by each of the following
context-free grammars.
\smallskip
\tparts{
   \vtop{\halign{$#$\hfil\cr
      S\PRODUCES aaSb\cr
      S\PRODUCES \EMPTYSTRING\cr
   }}
\qquad&
   \vtop{\halign{$#$\hfil\cr
      S\PRODUCES aSb\cr
      S\PRODUCES aaSb\cr
      S\PRODUCES \EMPTYSTRING\cr
   }}
\cr\noalign{\medskip}
   \vtop{\halign{$#$\hfil\cr
      S\PRODUCES TS\cr
      S\PRODUCES \EMPTYSTRING\cr
      T\PRODUCES aTb\cr
      T\PRODUCES \EMPTYSTRING\cr
   }}
\qquad&
   \vtop{\halign{$#$\hfil\cr
      S\PRODUCES ABA\cr
      A\PRODUCES aA\cr
      A\PRODUCES a\cr
      B\PRODUCES bB\cr
      B\PRODUCES cB\cr
      B\PRODUCES\EMPTYSTRING\cr
   }}
}
\smallskip

\problem For each of the following languages
find a context-free grammar that generates the language:
\pparts{
   \{a^nb^m\st n\ge m > 0\} &   \{a^nb^m\st n, m\in\N \}\cr
   \{a^nb^m\st n\ge0\AND m=n+1\} & \{a^nb^mc^n\st n,m\in\N \} \cr
   \{ a^nb^mc^k \st n=m+k \} & \{a^nb^m\st n\not=m\} \cr
   \{ a^nb^mc^rd^t\st n+m=r+t \} & \{ a^nb^mc^k \st n\not=m+k \} \cr
}

\problem Find a context-free grammar that generates the language
$\{w\in\{a,b\}^*\st n_a(w) > n_b(w)\}$.

\problem Find a context-free grammar that generates the language
$\{w\in\{a,b,c\}^*\st n_a(w) = n_b(w)\}$.

\problem A \nw{palindrome} is a string that reads the same
backwards and forwards, such as ``mom'', ``radar'', or
``aabccbccbaa''.  That is, $w$ is a palindrome if $w=w^R$.
Let $L=\{w\in\{a,b,c\}^*\st$ $w$ is a palindrome~$\}$.
Show that $L$ is a context-free language by finding a context-free
grammar that generates $L$.

\problem Let $\Sigma=\{\,\texttt{(},\,\texttt{)},\,\texttt{[},\,\texttt{]}\,\}$.  That is, $\Sigma$
is the alphabet consisting of the four symbols \texttt{(}, \texttt{)}, \texttt{[}, and~\texttt{]}.
Let $L$ be the language over $\Sigma$ consisting of strings
in which both parentheses and brackets are balanced.
For example, the string \texttt{([][()()])([])} is in $L$
but \texttt{[(])} is not.  Find a context-free grammar that generates the
language $L$.

\problem Suppose that $G$ and $H$ are context-free grammars.
Let $L=L(G)$ and let $M=L(H)$.  Explain how to construct
a context-free grammar for the language $LM$.  You do not
need to give a formal proof that your grammar is correct.

\problem Suppose that $G$ is a context-free grammar.
Let $L=L(G)$.  Explain how to construct
a context-free grammar for the language $L^*$.  You do not
need to give a formal proof that your grammar is correct.

\problem Suppose that $L$ is a context-free language.
Prove that $L^R$ is a context-free language.  (Hint:
Given a context-free grammar $G$ for $L$, make a new grammar, $G^R$,
by reversing the right-hand side of each of the production
rules in $G$.  That is, $A\PRODUCES w$ is a production rule in
$G$ if and only if $A\PRODUCES w^R$ is a production rule in $G^R$.)

\problem Define a \nw{left-regular grammar}
to be a context-free grammar in which the right-hand side of
every production rule is of one of the following forms:
the empty string; a single non-terminal symbol; or a non-terminal
symbol followed by a terminal symbol.  Show that a language is
regular if and only if it can be generated by a left-regular
grammar.  (Hint: Use the preceding exercise and Theorem~\ref{T-reggram}.)




\end{exercises}


\section{Application: BNF}\label{S-grammars-2}

Context-free grammars are used to describe some aspects of
the syntax of programming languages.  However, the notation
that is used for grammars in the context of programming languages
is somewhat different from the notation introduced in the
preceding section.  The notation that is used is called
\nw{Backus-Naur Form} or BNF.  It is named after computer
scientists John Backus and Peter Naur, who developed the
notation.  Actually, several variations of BNF exist.
I will discuss one of them here.  BNF can be used to describe
the syntax of natural languages, as well as programming languages,
and some of the examples in this section will deal with the
syntax of English.

Like context-free grammars, BNF grammars make use of production rules, non-terminals,
and terminals.  The non-terminals are usually given meaningful,
multi-character names.  Here, I will follow a common practice
of enclosing non-terminals in angle brackets, so that they can
be easily distinguished.  For example, \NT{noun} and \NT{sentence}
could be non-terminals in a BNF grammar for English, while
\NT{program} and \NT{if-statement} might be used in a BNF grammar
for a programming language.  Note that a BNF non-terminal
usually represents a meaningful \nw{syntactic category},
that is, a certain type of building block in the syntax of
the language that is being described, such as an adverb,
a prepositional phrase, or a variable declaration statement.
The terminals of a BNF grammar are the things that actually
appear in the language that is being described.  In the case
of natural language, the terminals are individual words.

In BNF production rules, I will use the symbol ``\BNFPRODUCES''
in place of the ``$\PRODUCES$'' that is used in context-free grammars.
BNF production rules are more powerful than the production rules in
context-free grammars.  That is, one BNF rule might be equivalent to 
several context-free grammar rules.  As for context-free grammars,
the left-hand side of a BNF production rule is a single 
non-terminal symbol.  The right hand side can include terminals
and non-terminals, and can also use the following notations,
which should remind you of notations used in regular expressions:
\smallskip
\IItem{$\bullet\,\,$}A vertical bar, \BNFALT, indicates a choice of
   alternatives.  For example,
   
\smallskip   
\centerline{\NT{digit} \BNFPRODUCES\ 0 \BNFALT\ 1 \BNFALT\ 2
          \BNFALT\ 3 \BNFALT\ 4 \BNFALT\ 5 \BNFALT\ 6 \BNFALT\ 7
          \BNFALT\ 8 \BNFALT\ 9}
          
\smallskip

\IItem{}indicates that the non-terminal \NT{digit} can be replaced
by any one of the terminal symbols 0, 1, \dots,~9.
\smallskip
\IItem{$\bullet\,\,$}Items enclosed in brackets are optional.  For example,

\smallskip
\centerline{\NT{declaration} \BNFPRODUCES\ \NT{type} \NT{variable}
                 [ = \NT{expression} ] ;}

\smallskip
\IItem{}says that \NT{declaration} can be replaced either
by ``\NT{type} \NT{variable}~;'' or by ``\NT{type} \NT{variable}
= \NT{expression}~;''.
(The symbols ``='' and ``;'' are terminal symbols in this rule.)

\smallskip
\IItem{$\bullet\,\,$}Items enclosed between ``['' and ``]\dots''
can be repeated zero or more times.  (This has the same effect
as a ``$*$''in a regular expression.)  For example,

\centerline{\NT{integer} \BNFPRODUCES\ \NT{digit} [ \NT{digit} ]\dots}
\smallskip

\IItem{}says that an \NT{integer} consists of a \NT{digit} followed
optionally by any number of additional \NT{digit}'s.  That is,
the non-terminal \NT{integer} can be replaced by \NT{digit} or
by \NT{digit}\NT{digit} or by \NT{digit}\NT{digit}\NT{digit}, and
so on.

\smallskip
\IItem{$\bullet\,\,$}Parentheses can be used as usual, for grouping. 
\smallskip

All these notations can be expressed in a context-free grammar
by introducing additional production rules.  For example, the
BNF rule ``\NT{sign}~\BNFPRODUCES\ +~\BNFALT~$-$'' is equivalent
to the two rules, ``\NT{sign}~\BNFPRODUCES~+''
and ``\NT{sign}~\BNFPRODUCES~$-$''.  A rule that contains an
optional item can also be replaced by two rules.  For example,

\smallskip
\centerline{\NT{declaration} \BNFPRODUCES\ \NT{type} \NT{variable}
                 [ = \NT{expression} ] ;}

\smallskip
\noindent can be replaced by the two rules

\smallskip
\centerline{\vbox{\halign{#\hfil\cr
         \NT{declaration} \BNFPRODUCES\ \NT{type} \NT{variable} ;\cr
         \NT{declaration} \BNFPRODUCES\ \NT{type} \NT{variable}
                  = \NT{expression}  ;\cr}}}
                  
\smallskip
\noindent In context-free grammars, repetition can be expressed by
using a recursive rule such as ``$S\PRODUCES aS$'', in which the
same non-terminal symbol appears both on the left-hand side and on the right-hand
side of the rule.  BNF-style notation using ``['' and ``]\dots'' can
be eliminated by replacing it with a new non-terminal symbol and adding
a recursive rule to allow that symbol to repeat zero or more times.
For example, the production rule

\smallskip
\centerline{\NT{integer} \BNFPRODUCES\ \NT{digit} [ \NT{digit} ]\dots}

\smallskip
\noindent can be replaced by three rules using a new non-terminal symbol
\NT{digit-list} to represent a string of zero or more \NT{digit}'s:

\smallskip
\centerline{\vbox{\halign{#\hfil\cr
   \NT{integer} \BNFPRODUCES\ \NT{digit} \NT{digit-list}\cr
   \NT{digit-list} \BNFPRODUCES\ \NT{digit} \NT{digit-list}\cr
   \NT{digit-list} \BNFPRODUCES\ $\EMPTYSTRING$\cr}}}

\medbreak

As an example of a complete BNF grammar, let's look at a BNF grammar for a very small
subset of English.  The start symbol for the grammar
is \NT{sentence}, and the terminal symbols are English words.
All the sentences that can be produced from this grammar
are syntactically correct English sentences, although you wouldn't
encounter many of them in conversation.  Here is the grammar:

\smallskip

\ \NT{sentence} \BNFPRODUCES\ \NT{simple-sentence} [ and \NT{simple-sentence} ]\dots
\smallskip

\ \NT{simple-sentence} \BNFPRODUCES\ \NT{nout-part} \NT{verb-part}
\smallskip

\ \NT{noun-part} \BNFPRODUCES\ \NT{article} \NT{noun} [ who \NT{verb-part} ]\dots
\smallskip

\ \NT{verb-part} \BNFPRODUCES\ \NT{intransitive-verb} \BNFALT\ ( \NT{transitive-verb} \NT{noun-part} )
\smallskip

\ \NT{article} \BNFPRODUCES\ the \BNFALT\ a
\smallskip

\ \NT{noun} \BNFPRODUCES\ man \BNFALT\ woman \BNFALT\ dog  \BNFALT\ cat  \BNFALT\ computer
\smallskip

\ \NT{intransitive-verb} \BNFPRODUCES\ runs \BNFALT\ jumps \BNFALT\ hides
\smallskip

\ \NT{transitive-verb} \BNFPRODUCES\ knows \BNFALT\ loves \BNFALT\ chases  \BNFALT\ owns

\smallskip

\noindent This grammar can generate sentences such as ``A dog chases the cat and
the cat hides'' and ``The man loves a woman who runs.''
The second sentence, for example, is generated by the derivation
\begin{align*}
    \NT{sentence}\ &\YIELDS\ \NT{simple-sentence}\\
       &\YIELDS\ \NT{noun-part}\ \NT{verb-part}\\
       &\YIELDS\ \NT{article}\ \NT{noun}\ \NT{verb-part}\\
       &\YIELDS\ \mbox{the}\ \NT{noun}\ \NT{verb-part}\\
       &\YIELDS\ \mbox{the}\ \mbox{man}\ \NT{verb-part}\\
       &\YIELDS\ \mbox{the}\ \mbox{man}\ \NT{transitive-verb}\ \NT{noun-part}\\
       &\YIELDS\ \mbox{the}\ \mbox{man}\ \mbox{loves}\ \NT{noun-part}\\
       &\YIELDS\ \mbox{the}\ \mbox{man}\ \mbox{loves}\ \NT{article}
              \ \NT{noun}\ \mbox{who}\ \NT{verb-part}\\
       &\YIELDS\ \mbox{the}\ \mbox{man}\ \mbox{loves}\ \mbox{a}
              \ \NT{noun}\ \mbox{who}\ \NT{verb-part}\\
       &\YIELDS\ \mbox{the}\ \mbox{man}\ \mbox{loves}\ \mbox{a}
              \ \mbox{woman}\ \mbox{who}\ \NT{verb-part}\\
       &\YIELDS\ \mbox{the}\ \mbox{man}\ \mbox{loves}\ \mbox{a}
              \ \mbox{woman}\ \mbox{who}\ \NT{intransitive-verb}\\
       &\YIELDS\ \mbox{the}\ \mbox{man}\ \mbox{loves}\ \mbox{a}
              \ \mbox{woman}\ \mbox{who}\ \mbox{runs}
\end{align*}

\medskip

BNF is most often used to specify the syntax of programming languages.
Most programming languages are not, in fact, context-free languages, and
BNF is not capable of expressing all aspects of their syntax.
For example, BNF cannot express the fact that a variable must
be declared before it is used or the fact that the number of
actual parameters in a subroutine call statement must match the number
of formal parameters in the declaration of the subroutine.
So BNF is used to express the context-free aspects of the syntax
of a programming language, and other restrictions on the syntax---such
as the rule about declaring a variable before it is used---are expressed
using informal English descriptions.

When BNF is applied to programming languages, the terminal symbols
are generally ``tokens,'' which are the minimal meaningful units in
a program.  For example, the pair of symbols \verb?<=? constitute a
single token, as does a string such as \texttt{"Hello World"}.
Every number is represented by a single token.  (The actual value
of the number is stored as a so-called ``attribute'' of the token,
but the value plays no role in the context-free syntax of the
language.) I will use the
symbol \textbf{\textsl{number}} to represent a numerical token.
Similarly, every variable name, subroutine name, or other identifier
in the program is represented by the same token, which I will denote
as \textbf{\textsl{ident}}.  One final complication:  Some symbols
used in programs, such as ``]'' and ``('', are also used with
a special meaning in BNF grammars.  When such a symbol occurs as
a terminal symbol, I will enclose it in double quotes.  For
example, in the BNF production rule

\smallskip
\centerline{ \NT{array-reference} \BNFPRODUCES\ 
   \textbf{\textsl{ident}} ``['' \NT{expression} ``]'' }

\smallskip
\noindent the ``['' and ``]'' are terminal symbols in the language
that is being described, rather than the BNF notation for an
optional item.  With this notation, here is part of a BNF
grammar that describes statements in the Java programming
language:

\smallskip

\ \NT{statement} \BNFPRODUCES\ \NT{block-statement} \BNFALT\ \NT{if-statement}
                                    \BNFALT\ \NT{while-statement}
 
\ \hbox to 1.5 in{} \BNFALT\ \NT{assignment-statement} \BNFALT\ \NT{null-statement}
\smallskip

\ \NT{block-statement} \BNFPRODUCES\ $\{$ [ \NT{statement} ]\dots\ $\}$
\smallskip

\ \NT{if-statement} \BNFPRODUCES\ if ``('' \NT{condition} ``)'' \NT{statement} [ else \NT{statement}~]
\smallskip

\ \NT{while-statement} \BNFPRODUCES\ while ``('' \NT{condition} ``)'' \NT{statement}
\smallskip

\ \NT{assignment-statement} \BNFPRODUCES\ \NT{variable} = \NT{expression} ;
\smallskip

\ \NT{null-statement} \BNFPRODUCES\ $\EMPTYSTRING$

\smallskip

\noindent The non-terminals \NT{condition}, \NT{variable}, and 
\NT{expression} would, of course, have to be defined by other
production rules in the grammar.  Here is a set of rules that
define simple expressions, made up of numbers, identifiers,
parentheses and the arithmetic operators +, $-$, $*$ and~/:

\smallskip

\ \NT{expression} \BNFPRODUCES\ \NT{term} [ [ + \BNFALT\ $-$ ] \NT{term} ]\dots
\smallskip

\ \NT{term} \BNFPRODUCES\ \NT{factor} [ [ $*$ \BNFALT\ / ] \NT{factor} ]\dots
\smallskip

\ \NT{factor} \BNFPRODUCES\ \textbf{\textsl{ident}} \BNFALT\ \textbf{\textsl{number}} \BNFALT\ ``('' \NT{expression} ``)''
\smallskip

\noindent The first rule says that an \NT{expression} is a sequence of
one or more \NT{term}'s, separated by plus or minus signs.
The second rule defines a \NT{term} to be a sequence of one or more
\NT{factors}, separated by multiplication or division operators.
The last rule says that a \NT{factor} can be either an identifier
or a number or an \NT{expression} enclosed in parentheses.
This small BNF grammar can generate expressions 
such as ``$3*5$'' and ``$x*(x+1) - 3/(z+2*(3-x)) + 7$''.
The latter expression is made up of three terms: $x*(x+1)$,
$3/(z+2*(3-x))$, and~$7$.  The first of these terms is made
up of two factors, $x$ and $(x+1)$.  The factor $(x+1)$ consists
of the expression $x+1$ inside a pair of parentheses.

The nice thing about this grammar is that the precedence rules
for the operators are implicit in the grammar.  For example, according
to the grammar, the expression $3+5*7$ is seen as \NT{term} + \NT{term}
where the first term is $3$ and the second term is $5*7$.
The $5*7$ occurs as a group, which must be evaluated before the
result is added to $3$.  Parentheses can change the order of
evaluation.  For example, $(3+5)*7$ is generated by the grammar
as a single \NT{term} of the form $\NT{factor}*\NT{factor}$.
The first \NT{factor} is $(3+5)$.  When $(3+5)*7$ is evaluated,
the value of $(3+5)$ is computed first and then multiplied
by~$7$.  This is an example of how a grammar that describes
the syntax of a language can also reflect its meaning.

\medskip

Although this section has not introduced any really new ideas
or theoretical results, I hope it has demonstrated how 
context-free grammars can be applied in practice.  

\begin{exercises}

\problem One of the examples in this section was a grammar for
a subset of English.  Give five more examples of sentences that
can be generated from that grammar.  Your examples should, collectively,
use all the rules of the grammar.

\problem Rewrite the example BNF grammar for a subset of English as
a context-free grammar.

\problem Write a single BNF production rule that is equivalent to
the following context-free grammar:
\begin{align*}
    S &\PRODUCES aSa\\
    S &\PRODUCES bB\\
    B &\PRODUCES bB\\
    B &\PRODUCES \EMPTYSTRING
\end{align*}

\problem Write a BNF production rule that specifies the syntax of
real numbers, as they appear in programming languages such as Java and C.  
Real numbers can include a sign, a decimal point and an exponential part.
Some examples are:  17.3, .73, 23.1e67, $-$1.34E$-$12, +0.2, 100E+100

\problem Variable references in the Java programming language can be 
rather complicated.  Some examples include:
$x$, $list.next$, $A[7]$, $a.b.c$, $S[i+1].grid[r][c].red$, \dots.
Write a BNF production rule for Java variables.
You can use the token \textbf{\textsl{ident}} and the non-terminal
\NT{expression} in your rule.
\
\problem Use BNF to express the syntax of the try\dots catch statement in the
Java programming language.

\problem Give a BNF grammar for compound propositions made up
of propositional variables, parentheses, and the logical operators
$\AND$, $\OR$, and $\NOT$.  Use the non-terminal symbol \NT{pv} to represent
a propositional variable.  You do not have to give a definition of
\NT{pv}.

\end{exercises}


\section{Parsing and Parse Trees}\label{S-grammars-3}

Suppose that $G$ is a grammar for the language $L$.  That is, 
$L=L(G)$.  The grammar $G$ can be used to generate strings in
the language~$L$.  In practice, though, we often start with a string
which might or might not be in~$L$, and the problem is
to determine whether the string is in the language and, if so,
how it can be generated by~$G$.  The goal is to find a derivation
of the string, using the production rules of the grammar, or to
show that no such derivation exists.  This is known as \nw{parsing}
the string.  When the string is a computer program or a sentence
in a natural language, parsing the string is an essential step
in determining its meaning.

As an example that we will use throughout
this section, consider the language that consists of arithmetic
expressions containing parentheses, the binary operators $+$ and $*$,
and the variables $x$, $y$, and $z$.  Strings in this language
include $x$, $x+y*z$, and $((x+y)*y)+z*z$.  Here is a context-free
grammar that generates this language:
\begin{align*}
   E&\PRODUCES E+E\\
   E&\PRODUCES E*E\\
   E&\PRODUCES (E)\\
   E&\PRODUCES x\\
   E&\PRODUCES y\\
   E&\PRODUCES z
\end{align*}
Call the grammar described by these production rules $G_1$.
The grammar $G_1$ says that $x$, $y$, and $z$ are expressions, and that
you can make new expressions by adding two expressions, by multiplying
two expressions, and by enclosing an expression in parentheses.
(Later, we'll look at other grammars for the same language---ones that
turn out to have certain advantages over $G_1$.)

Consider the string $x+y*z$.  To show that this string is in the
language $L(G_1)$, we can exhibit a derivation of the string
from the start symbol $E$.  For example:
\begin{align*}
   E &\YIELDS E+E\\
     &\YIELDS E+E*E\\
     &\YIELDS E+y*E\\
     &\YIELDS x+y*E\\
     &\YIELDS x+y*z
\end{align*}
This derivation shows that the string $x+y*z$ is in fact in $L(G_1)$.
Now, this string has many other derivations.  At each step in the
derivation, there can be a lot of freedom about which rule in the
grammar to apply next.  Some of this freedom is clearly not very
meaningful.  When faced with the string $E+E*E$ in the above example,
the order in which we replace the $E\text{'}s$ with the variables $x$, $y$,
and $z$ doesn't much matter.  To cut out some of this meaningless
freedom, we could agree that in each step of a derivation, the
non-terminal symbol that is replaced is the leftmost non-terminal
symbol in the string.  A derivation in which this is true is
called a \nw{left derivation}.  The following left derivation
of the string $x+y*z$ uses the same production rules as the previous
derivation, but it applies them in a different order:
\begin{align*}
   E &\YIELDS E+E\\
     &\YIELDS x+E\\
     &\YIELDS x+E*E\\
     &\YIELDS x+y*E\\
     &\YIELDS x+y*z
\end{align*}
It shouldn't be too hard to convince yourself that any string that
has a derivation has a left derivation (which can be obtained
by changing the order in which production rules are applied).

We have seen that the same string might have several different derivations.
We might ask whether it can have several different left derivations.
The answer is that it depends on the grammar.  A context-free
grammar $G$ is said to be \nw[ambiguous grammar]{ambiguous}\index{grammar!ambiguous}
if there is a string $w\in L(G)$ such that $w$ has more than one
left derivation according to the grammar $G$.

Our example grammar $G_1$ is ambiguous.  In fact, in addition to the
left derivation given above, the string $x+y*z$ has the alternative
left derivation
\begin{align*}
   E &\YIELDS E*E\\
     &\YIELDS E+E*E\\
     &\YIELDS x+E*E\\
     &\YIELDS x+y*E\\
     &\YIELDS x+y*z
\end{align*}
In this left derivation of the string $x+y*z$, the first production
rule that is applied is $E\PRODUCES E*E$.  The first $E$ on the right-hand
side eventually yields ``$x+y$'' while the second yields ``$z$''.
In the previous left derivation, the first production rule that was
applied was $E\PRODUCES E+E$, with the first $E$ on the right yielding
``$x$'' and the second $E$ yielding ``$y*z$''.  If we think in terms
of arithmetic expressions, the two left derivations lead to
two different interpretations of the expression $x+y*z$.  In one
interpretation, the $x+y$ is a unit that is multiplied by $z$.
In the second interpretation, the $y*z$ is a unit that is added to $x$.
The second interpretation is the one that is correct according to
the usual rules of arithmetic.  However, the grammar allows either
interpretation.  The ambiguity of the grammar allows the string to
be parsed in two essentially different ways, and only one of the
parsings is consistent with the meaning of the string.  Of course,
the grammar for English is also ambiguous.  In a famous example,
it's impossible to tell whether a ``pretty girls' camp'' is
meant to describe a pretty camp for girls or a camp for pretty girls.

When dealing with artificial languages such as programming languages,
it's better to avoid ambiguity.
The grammar $G_1$ is perfectly correct in that it generates the correct
set of strings, but in a practical situation where we are interested
in the meaning of the strings, $G_1$ is not the right grammar for
the job.  There are other grammars that generate the same language
as $G_1$.  Some of them are unambiguous grammars that better reflect
the meaning of the strings in the language.  For example, the
language $L(G_1)$ is also generated by the BNF grammar
\begin{align*}
   E\ &\BNFPRODUCES\ T\ [\ +\ T\ ]\dots\\
   T\ &\BNFPRODUCES\ F\ [\ *\ F\ ]\dots\\
   F\ &\BNFPRODUCES\ \text{``(''}\ E\ \text{``)''}\ \BNFALT\ x\ \BNFALT\ y\ \BNFALT\ z
\end{align*} 
This grammar can be translated into a standard context-free grammar, which
I will call $G_2$:
\begin{align*}
   E &\PRODUCES TA\\
   A &\PRODUCES +TA\\
   A &\PRODUCES \EMPTYSTRING\\
   T &\PRODUCES FB\\
   B &\PRODUCES *FB\\
   B &\PRODUCES \EMPTYSTRING\\
   F &\PRODUCES (E)\\
   F &\PRODUCES x\\
   F &\PRODUCES y\\
   F &\PRODUCES z
\end{align*}
The language generated by
$G_2$ consists of all legal arithmetic expressions made up of 
parentheses, the operators $+$ and $-$, and the variables $x$, $y$,
and $z$.  That is, $L(G_2)=L(G_1)$.  However, $G_2$ is an unambiguous
grammar.  Consider, for example, the string $x+y*z$.  Using the
grammar $G_2$, the only left derivation for this string is:
\begin{align*}
   E &\YIELDS TA\\
     &\YIELDS FBA\\
     &\YIELDS xBA\\
     &\YIELDS xA\\
     &\YIELDS x+TA\\
     &\YIELDS x+FBA\\
     &\YIELDS x+yBA\\
     &\YIELDS x+y*FBA\\
     &\YIELDS x+y*zBA\\
     &\YIELDS x+y*zA\\
     &\YIELDS x+y*z
\end{align*}
There is no choice about the first step in this derivation, since the
only production rule with $E$ on the left-hand side is $E\PRODUCES TA$.
Similarly, the second step is forced by the fact that there is only
one rule for rewriting a $T$.  In the third step, we must replace
an $F$.  There are four ways to rewrite $F$, but only one way to produce
the $x$ that begins the string $x+y*z$, so we apply the rule $F\PRODUCES x$.
Now, we have to decide what to do with the $B$ in $xBA$.  There two rules
for rewriting $B$, $B\PRODUCES *FB$ and $B\PRODUCES\EMPTYSTRING$.  However,
the first of these rules introduces a non-terminal, $*$, which does not
match the string we are trying to parse.  So, the only choice is to
apply the production rule $B\PRODUCES\EMPTYSTRING$.  In the next step
of the derivation, we must apply the rule $A\PRODUCES +TA$ in order to 
account for the $+$ in the string $x+y*z$.  Similarly, each of the 
remaining steps in the left derivation is forced.

\medbreak

The fact that $G_2$ is an unambiguous grammar means that at each
step in a left derivation for a string $w$, there is only one production
rule that can be applied which will lead ultimately to a correct
derivation of~$w$.  However, $G_2$ actually satisfies a much stronger
property:  at each step in the left derivation of $w$, we can tell which
production rule has to be applied by looking ahead at the next
symbol in~$w$.  We say that $G_2$ is an \nw{LL(1) grammar}.
(This notation means that we can read a string from \textbf{L}eft to
right and construct a \textbf{L}eft derivation of the string by
looking ahead at most \textbf{1} character in the string.)
Given an LL(1) grammar for a language, it is fairly straightforward
to write a computer program that can parse strings in that language.
If the language is a programming language, then parsing is one of the
essential steps in translating a computer program into machine language.
LL(1) grammars and parsing programs that use them are often studied
in courses in programming languages and the theory of compilers.

Not every unambiguous context-free grammar is an LL(1) grammar.  Consider, for
example, the following grammar, which I will call $G_3$:
\begin{align*}
   E &\PRODUCES E + T\\
   E &\PRODUCES T\\
   T &\PRODUCES T*F\\
   T &\PRODUCES F\\
   F &\PRODUCES (E)\\
   F &\PRODUCES x\\
   F &\PRODUCES y\\
   F &\PRODUCES z
\end{align*}
This grammar generates the same language as $G_1$ and $G_2$,
and it is unambiguous.  However, it is not possible to construct
a left derivation for a string according to the grammar $G_3$ by
looking ahead one character in the string at each step.  
The first step in any left derivation must be either
$E\YIELDS E+T$ or $E\YIELDS T$.  But how can we decide which of
these is the correct first step?
Consider the strings $(x+y)*z$ and $(x+y)*z+z*x$, which are both
in the language $L(G_3)$.  For the string $(x+y)*z$, the
first step in a left derivation must be $E\YIELDS T$, while 
the first step in a left derivation of $(x+y)*z+z*x$ must be
$E\YIELDS E+T$.  However, the first seven characters of the strings
are identical, so clearly looking even seven characters ahead is not
enough to tell us which production rule to apply.  In fact,
similar examples show that looking ahead any given finite number of
characters is not enough.

However, there is an alternative parsing procedure that will work
for $G_3$.  This alternative method of parsing a string produces
a \nw{right derivation} of the string, that is, a derivation in
which at each step, the non-terminal symbol that is replaced is
the rightmost non-terminal symbol in the string.  Here, for example,
is a right derivation of the string $(x+y)*z$ according to the
grammar $G_3$:
\begin{align*}
  E &\YIELDS T\\
    &\YIELDS T*F\\
    &\YIELDS T*z\\
    &\YIELDS F*z\\
    &\YIELDS (E)*z\\
    &\YIELDS (E+T)*z\\
    &\YIELDS (E+F)*z\\
    &\YIELDS (E+y)*z\\
    &\YIELDS (T+y)*z\\
    &\YIELDS (F+y)*z\\
    &\YIELDS (x+y)*z
\end{align*}
The parsing method that produces this right derivation produces
it from ``bottom to top.''  That is, it begins with
the string $(x+y)*z$ and works backward to the start symbol $E$,
generating the steps of the right derivation in reverse order.
The method works because $G_3$ is what is called an
\nw{LR(1) grammar}.  That is, roughly, it is possible to read
a string from \textbf{L}eft to right and produce a \textbf{R}ight
derivation of the string, by looking ahead at most \textbf{1} symbol at
each step.  Although LL(1) grammars are easier for people to work
with, LR(1) grammars turn out to be very suitable for machine
processing, and they are used as the basis for the parsing
process in many compilers.

LR(1) parsing uses a \nw{shift/reduce} algorithm.  Imagine a
cursor or current position that moves through the string that
is being parsed.  We can visualize the cursor as a vertical
bar, so for the string $(x+y)*z$, we start with the
configuration $|(x+y)*z$.  A \textit{shift} operation simply
moves the cursor one symbol to the right.  For example,
a shift operation would convert $|(x+y)*z$ to $(|x+y)*z$,
and a second shift operation would convert that to
$(x|+y)*z$.  In a reduce
operation, one or more symbols immediately to the left of
the cursor are recognized as the right-hand side of one of
the production rules in the grammar.  These symbols are removed
and replaced by the left-hand side of the production rule.
For example, in the configuration $(x|+y)*z$, the $x$ to the left
of the cursor is the right-hand side of the production rule
$F\PRODUCES x$, so we can apply a reduce operation and replace
the $x$ with $F$, giving $(F|+y)*z$.  This first reduce operation
corresponds to the last step in the right derivation of the
string, $(F+y)*z\YIELDS (x+y)*z$.  Now the $F$ can be recognized
as the right-hand side of the production rule $T\PRODUCES F$,
so we can replace the $F$ with $T$, giving $(T|+y)*z$.
This corresponds to the next-to-last step in the right
derivation, $(T+y)*z\YIELDS (F+y)*z$.

At this point, we have the configuration $(T|+y)*z$.  The $T$
could be the right-hand side of the production rule $E\PRODUCES T$.
However, it could also conceivably come from the rule $T\PRODUCES T*F$.
How do we know whether to reduce the $T$ to $E$ at this point or to
wait for a $*F$ to come along so that we can reduce $T*F\,$?
We can decide by looking ahead at the next character after the
cursor.  Since this character is a $+$ rather than a $*$,
we should choose the reduce operation that replaces $T$ with $E$,
giving $(E|+y)*z$.  What makes $G_3$ an LR(1) grammar is the fact
that we can always decide what operation to apply by looking
ahead at most one symbol past the cursor.

After a few more shift and reduce operations, the configuration
becomes $(E)|*z$, which we can reduce to $T|*z$ by applying the
production rules $F\PRODUCES (E)$ and $T\PRODUCES F$.
Now, faced with $T|*z$, we must once again decide between
a shift operation and a reduce operation that applies the
rule $E\PRODUCES T$.  In this case, since the next character is
a $*$ rather than a $+$, we apply the shift operation, giving
$T*|z$.  From there we get, in succession, $T*z|$,
$T*F|$, $T|$, and finally $E|$.  At this point, we have reduced
the entire string $(x+y)*z$ to the start symbol of the grammar.
The very last step, the reduction of $T$ to $E$ corresponds to
the first step of the right derivation, $E\YIELDS T$.

In summary, LR(1) parsing transforms a string into the
start symbol of the grammar by a sequence of shift and
reduce operations.  Each reduce operation corresponds to a
step in a right derivation of the string, and these steps
are generated in reverse order.  Because the steps in the
derivation are generated from ``bottom to top,'' LR(1)
parsing is a type of \nw{bottom-up parsing}.  LL(1) parsing,
on the other hand, generates the steps in a left derivation
from ``top to bottom'' and so is a type of \nw{top-down parsing}.

\medbreak

Although the language generated by a context-free grammar
is defined in terms of derivations, there is another way of
presenting the generation of a string that is often more useful.
A \nw{parse tree} displays the generation of a string from
the start symbol of a grammar as a two dimensional diagram.
Here are two parse trees that show two derivations of the
string x+y*z according to the grammar $G_1$, which was given
at the beginning of this section:
\bigskip
\centerline{\scaledeps{2.5truein}{fig-5-1}}

\noindent A parse tree is made up of terminal and non-terminal symbols,
connected by lines.  The start symbol is at the top, or ``root,'' of
the tree.  Terminal symbols are at the lowest level, or ``leaves,'' of
the tree.  (For some reason, computer scientists traditionally
draw trees with leaves at the bottom and root at the top.)
A production rule $A\PRODUCES w$ is represented
in a parse tree by the symbol $A$ lying above all the symbols in $w$,
with a line joining $A$ to each of the symbols in $w$.  For
example, in the left parse tree above, the root,
$E$, is connected to the symbols $E$, $+$, and $E$, and this
corresponds to an application of the production rule
$E\PRODUCES E+E$.

It is customary to draw a parse tree with the string of non-terminals
in a row across the bottom, and with the rest of the tree built on
top of that base.  Thus, the two parse trees shown above might
be drawn as:

\bigskip
\centerline{\scaledeps{2.5truein}{fig-5-2}}

Given any derivation of a string, it is possible to construct
a parse tree that shows each of the steps in that derivation.
However, two different derivations can give rise to the same
parse tree, since the parse tree does not show the order in
which production rules are applied.  For example, the parse
tree on the left, above, does not show whether the production
rule $E\PRODUCES x$ is applied before or after the production
rule $E\PRODUCES y$.  However, if we restrict our attention to left
derivations, then we find that each parse tree corresponds to
a unique left derivation and \textit{vice versa}.  I will state this
fact as a theorem, without proof.  A similar result holds for
right derivations.

\begin{theorem}
Let $G$ be a context-free grammar.  There is a one-to-one correspondence
between parse trees and left derivations based on the grammar $G$.
\end{theorem}

Based on this theorem, we can say that a context-free grammar $G$
is ambiguous if and only if there is a string $w\in L(G)$ which has
two parse trees.



\begin{exercises}

\problem Show that each of the following grammars is ambiguous by finding
a string that has two left derivations according to the grammar:
\tparts{
   \vtop{\halign{$#$\hfil\cr
      S\PRODUCES SS\cr
      S\PRODUCES aSb\cr
      S\PRODUCES bSa\cr
      S\PRODUCES\EMPTYSTRING\cr
   }}\quad&
   \vtop{\halign{$#$\hfil\cr
      S\PRODUCES ASb\cr
      S\PRODUCES \EMPTYSTRING\cr
      A\PRODUCES aA\cr
      A\PRODUCES a\cr
   }}\cr
}

\problem Consider the string $z+(x+y)*x$.  Find a left derivation
of this string according to each of the grammars $G_1$, $G_2$, and
$G_3$, as given in this section.

\problem Draw a parse tree for the string $(x+y)*z*x$ according to
each of the grammars $G_1$, $G_2$, and $G_3$, as given in this section.

\problem Draw three different parse trees for the string
$ababbaab$ based on the grammar given in part a) of exercise 1.

\problem Suppose that the string $abbcabac$ has the following parse
tree, according to some grammar $G$:

\centerline{\scaledeps{2in}{fig-5-3}}
\medskip

\ppart List five production rules that must be rules in the grammar $G$,
given that this is a valid parse tree.
\ppart Give a left derivation for the string $abbcabac$ according to the
grammar $G$.
\ppart Give a right derivation for the string $abbcabac$ according to the
grammar $G$.

\problem Show the full sequence of shift and reduce operations
that are used in the LR(1) parsing of the string $x+(y)*z$ according
to the grammar $G_3$, and give the corresponding right derivation
of the string.

\problem This section showed how to use LL(1) and LR(1) parsing to
find a derivation of a string in the language $L(G)$ generated by
some grammar $G$.  How is it possible to use LL(1) or LR(1) parsing
to determine for an arbitrary string $w$ whether $w\in L(G)\,$?
Give an example.

\end{exercises}


\section{Pushdown Automata}\label{S-grammars-3b}

In the previous chapter, we saw that there is a neat correspondence
between regular expressions and finite automata.  That is, a language
is generated by a regular expression if and only if that language is
accepted by a finite automaton.  Finite automata come in two types,
deterministic and nondeterministic, but the two types of finite
automata are equivalent in terms of their ability to recognize
languages.  So, the class of regular languages\index{regular language} can be defined in
two ways: either as the set of languages that can be generated by
regular expressions or as the set of languages that can be recognized
by finite automata (either deterministic or nondeterministic).

In this chapter, we have introduced the class of context-free languages,
and we have considered how context-free grammars can be used to
generate context-free languages.  You might wonder whether there is any
type of automaton that can be used to recognize context-free languages.
In fact, there is:  The abstract machines known as 
\nw[pushdown automaton]{pushdown automata} can be used to define
context-free languages.  That is, a language is context-free if and only
if there is a pushdown automaton that accepts that language.

\medbreak

A pushdown automaton is essentially a finite automaton with an auxiliary 
data structure known as a \nw{stack}.  A stack consists of
a finite list of symbols.  Symbols can be added to and removed from the
list, but only at one end of the list.  The end of the list where items
can be added and removed is called the \nw[none]{top} of the stack.
The list is usually visualized as a vertical ``stack'' of symbols,
with items being added and removed at the top.  Adding a symbol at
the top of the stack is referred to as \nw[push operation on a stack]{pushing} a symbol onto 
the stack, and removing a symbol is referred to as \nw[pop operation on a stack]{popping}
an item from the stack.  During each step of its computation, 
a pushdown automaton is capable of doing several
push and pop operations on its stack (this in addition to possibly reading
a symbol from the input string that is being processed by the automaton).

Before giving a formal definition of pushdown automata, we will look
at how they can be represented by transition diagrams.  A diagram of
a pushdown automaton is similar to a diagram for an NFA, except that
each transition in the diagram can involve stack operations.  We will
use a label of the form $\sigma,x$/$y$ on a transition to mean
that the automaton consumes $\sigma$ from its input string, pops
$x$ from the stack, and pushes $y$ onto the stack.  $\sigma$ can be
either $\varepsilon$ or a single symbol.
$x$ and $y$ are strings, possibly empty. (When a string $x=a_1a_2\dots a_k$ is
pushed onto a stack, the symbols are pushed in the order $a_k,\dots,a_1$, so that
$a_1$ ends up on the top of the stack; for $y=b_1b_2\dots b_n$ to be popped
from the stack, $b_1$ must be the top symbol on the stack, followed by $b_2$, etc.) 
For example, consider the following transition diagram for a pushdown automaton:

\medskip
\centerline{\scaledeps{2truein}{fig-5-pa-1}}
\smallskip

This pushdown automaton has start state $q_0$ and one accepting
state, $q_1$.  It can read strings over the alphabet $\Sigma=\{a,b\}$.
The transition from $q_0$ to $q_0$, labeled with $a,\varepsilon$/1,
means that if the machine is in state $q_0$, then it can read an
$a$ from its input string, pop nothing from the stack, push 1 onto
the stack, and remain in state $q_0$.  Similarly, the transition
from $q_1$ to $q_1$ means that if the machine is in state $q_1$,
it can read a $b$ from its input string, pop a 1 from the stack,
and push nothing onto the stack.  Finally, the transition
from state $q_0$ to $q_1$, labeled with $\varepsilon,\varepsilon$/$\varepsilon$,
means that the machine can transition from state $q_0$ to state $q_1$
without reading, pushing, or popping anything.

Note that the automation can follow transition $b,1$/$\varepsilon$ only
if the next symbol in the input string is $b$ and if $1$ is on the 
top of the stack.  When it makes the transition, it consumes the $b$ from
input and pops the $1$ from the stack.  Since in this case, the automaton
pushes $\varepsilon$ (that is, no symbols at all) onto the stack, the net
change in the stack is simply to pop the 1.

We have to say what it means for this pushdown automaton to accept
a string.  For $w\in\{a,b\}^*$, we say that the pushdown automaton
accepts $w$ if and only if it is possible for the machine to start
in its start state, $q_0$, read all of $w$, and finish in the
accepting state, $q_1$, with an empty stack.  Note in particular that
it is not enough for the machine to finish in an accepting state---it
must also empty the stack.\footnote{We could relax this restriction
and require only that the machine finish in an accepting state after
reading the string $w$, without requiring that the stack be empty.
In fact, using this definition of accepting would not change the
class of languages that are accepted by pushdown automata.}

It's not difficult to see that with this definition, the language
accepted by our pushdown automaton is $\{a^nb^n\st n\in\N\}$.
In fact, given the string $w=a^kb^k$, the machine can process this
string by following the transition from $q_0$ to $q_0$ $k$ times.
This will consume all the $a$'s and will push $k$ 1's onto the stack.
The machine can then jump to state $q_1$ and follow the transition from
$q_1$ to $q_1$ $k$ times.  Each time it does so, it consumes one $b$ from
the input and pops one 1 from the stack.  At the end, the input has been
completely consumed and the stack is empty.  So, the string $w$ is
accepted by the automaton.  Conversely, this pushdown automaton \emph{only}
accepts strings of the form $a^kb^k$, since the only way that the
automaton can finish in the accepting state, $q_1$, is to follow
the transition from $q_0$ to $q_0$ some number of times, reading
$a$'s as it does so, then jump at some point to $q_1$, and then
follow the transition from $q_1$ to $q_1$ some number of times,
reading $b$'s as it does so.  This means that an accepted string
must be of the form $a^kb^\ell$ for some $k,\ell\in\N$.  However, in
reading this string, the automaton pushes $k$ 1's onto the stack
and pops $\ell$ 1's from the stack.  For the stack to end up empty,
$\ell$ must equal $k$, which means that in fact the string is of
the form $a^kb^k$, as claimed.  

\medbreak

Here are two more examples.  These pushdown automata use the capability
to push or pop more than one symbol at a time:

\medskip
\centerline{\scaledeps{4truein}{fig-5-pa-1b}}
\smallskip

\noindent The atomaton on the left accepts the language
$\{a^nb^m\st n\le m\le 2*n\}$.  Each time it reads an $a$,
it pushes either one or two 1's onto the stack, so that
after reading $n$ $a$'s, the number of 1's on the stack
is between $n$ and $2*n$.  If the machine then jumps to state
$q_1$, it must be able to read exactly enough $b$'s to empty 
the stack, so any string accepted by this machine must
be of the form $a^nb^m$ with $n\le m\le 2*n$.  Conversely,
any such string can be accepted by the machine. Similarly,
the automaton on the right above accepts the
language $\{a^nb^m\st n/2 \le m \le n\}$.
To accept $a^nb^m$, it must push $n$ 1'a onto the
stack and then pop one or two 1's for each b; this
can succeed only if the number of $b$'s is between
$n/2$ and $n$.



\medbreak

Note that an NFA can be considered to be a pushdown automaton that
does not make any use of its stack.  This means that any language
that can be accepted by an NFA (that is, any regular language) can
be accepted by a pushdown automaton.
Since the language $\{a^nb^n\st n\in\N\}$ is context-free but not regular,
and since it is accepted by the above pushdown automaton,
we see that pushdown automata are capable of recognizing context-free languages
that are not regular and so that pushdown automata are strictly more
powerful than finite automata.

\bigbreak

Although it is not particularly illuminating, we can give a formal
definition of pushdown automaton.  The definition does at least
make it clear that the set of symbols that can be used on the
stack is not necessarily the same as the set of symbols that can
be used as input.

\begin{definition}
A pushdown automaton\index{pushdown automaton} $M$ is specified by six components
$M=(Q,\Sigma,\Lambda,q_0,\partial,F)$ where
\begin{itemize}
\item $Q$ is a finite set of states.
\item $\Sigma$ is an alphabet.  $\Sigma$ is the \nw[none]{input alphabet} for $M$.
\item $\Lambda$ is an alphabet.  $\Lambda$ is the \nw[none]{stack alphabet} for $M$.
\item $q_0\in Q$ is the \nw[none]{start state} of $M$.
\item $F\SUB Q$ is the set of \nw[none]{final} or \nw[none]{accepting} states in $M$.
\item $\partial$ is the set of transitions in $M$.  $\partial$ can be taken
to be a finite subset of the set 
$(Q\times(\Sigma\cup\{\varepsilon\})\times\Lambda^*\big)\times\big(Q\times\Lambda^*\big)$.
An element $\big((q_1,\sigma,x),(q_2,y)\big)$ of $\partial$ represents a transition from
state $q_1$ to state $q_2$ in which $M$ reads $\sigma$ from its input string,
pops $x$ from the stack, and pushes $y$ onto the stack.
\end{itemize}
\end{definition}

We can then define the language $L(M)$ accepted by a pushdown
automaton $M=(Q,\Sigma,\Lambda,q_0,\partial,F)$ to be the set
$L(M)=\{w\in\Sigma^*\ |$ starting from state $q_0$, it is possible for $M$ to
read all of $w$ and finish in some state in $F$ with an empty stack$\}$.
With this definition, the class of languages accepted by pushdown automata
is the same as the class of languages generated by context-free grammars.

\begin{theorem}
Let $\Sigma$ be an alphabet, and let $L$ be a language over $L$.  Then
$L$ is context-free if and only if there is a pushdown automaton whose
input alphabet is $\Sigma$ such that $L=L(M)$.
\end{theorem}

We will not prove this theorem, but we do discuss how one direction can
be proved.  Suppose that $L$ is a context-free language over an alphabet
$\Sigma$.  Let $G=(V,\Sigma,P,S)$ be a context-free grammar for $L$.
Then we can construct a pushdown automaton $M$ that accepts $L$.  In fact,
we can take $M=(Q,\Sigma,\Lambda,q_0,\partial,F)$ where
$Q=\{q_0,q_1\}$, $\Lambda=\Sigma\cup V$, $F=\{q_1\}$, and
$\partial$ contains transitions of the forms

\begin{enumerate}
\item $\big((q_0,\varepsilon,\varepsilon),(q_1,S)\big)$;
\item $\big((q_1,\sigma,\sigma),(q_1,\varepsilon)\big)$, for $\sigma\in\Sigma$; and
\item $\big((q_1,\varepsilon,A),(q_1,x)\big)$, for each production $A\PRODUCES x$ in $G$.
\end{enumerate}

The transition $\big((q_0,\varepsilon,\varepsilon),(q_1,S)\big)$ lets $M$
move from the start state $q_0$ to the accepting state $q_1$ while reading
no input and pushing $S$ onto the stack.  This is the only possible first move
by $M$.  

A transition of the form $\big((q_1,\sigma,\sigma),(q_1,\varepsilon)\big)$, for $\sigma\in\Sigma$
allows $M$ to read $\sigma$ from its input string, provided there is a $\sigma$
on the top of the stack.  Note that if $\sigma$ is at the top of the stack, then this
transition is \emph{only} transition that applies.  Effectively, any terminal symbol
that appears at the top of the stack must be matched by the same symbol in the
input string, and the transition rule allows $M$ to consume the symbol from the
input string and remove it from the stack at the same time.

A transition of the third form, $\big((q_1,\varepsilon,A),(q_1,x)\big)$ can
be applied if and only if the non-terminal symbol $A$ is at the top of
the stack.  $M$ consumes no input when this rule is applied, but $A$ is
replaced on the top of the stack by the string on the right-hand
side of the production rule $A\PRODUCES x$.  Since the grammar $G$ can
contain several production rules that have $A$ as their left-hand side,
there can be several transition rules in $M$ that apply when $A$ is on the
top of the stack.  This is the only source of nondeterminism in $M$; note
that is also the source of nondeterminism in $G$.

The proof that $L(M)=L(G)$ follows from the fact that
a computation of $M$ that accepts a string $w\in\Sigma^*$ corresponds in
a natural way to a left derivation of $w$ from $G$'s start symbol, $S$.
Instead of giving a proof of this fact, we look at an example.
Consider the following context-free grammar:
\begin{align*}
  S&\PRODUCES AB\\
  A&\PRODUCES aAb\\
  A&\PRODUCES \varepsilon\\
  B&\PRODUCES bB\\
  B&\PRODUCES b
\end{align*}
This grammar generates the language $\{a^nb^m\st m > n\}$.  The pushdown
automaton constructed from this grammar by the procedure given above has
the following set of transition rules:
\begin{align*}
  &\big((q_0,\varepsilon,\varepsilon),(q_1,S)\big)\\
  &\big((q_1,a,a),(q_1,\varepsilon)\big)\\
  &\big((q_1,b,b),(q_1,\varepsilon)\big)\\
  &\big((q_1,\varepsilon,S),(q_1,AB)\big)\\
  &\big((q_1,\varepsilon,A),(q_1,aAb)\big)\\
  &\big((q_1,\varepsilon,A),(q_1,\varepsilon)\big)\\
  &\big((q_1,\varepsilon,B),(q_1,bB)\big)\\
  &\big((q_1,\varepsilon,B),(q_1,b)\big)\\
\end{align*}
Suppose that the automaton is run on the input $aabbbb$.  We can trace the
sequence of transitions that are applied in a computation that accepts this
input, and we can compare that computation to a left derivation of the
string:

{\def\stack#1{\vbox{\halign{##\hfil\cr#1}}}

\bigskip
\halign to \hsize{\qquad$#$\hfil&&$#$\hfil\cr
  \hbox{\bf\underbar{Transition}}                  &\qquad\hbox{\stack{\bf\underbar{Input}\cr\bf\underbar{Consumed}\cr}}  &
                          \qquad\hbox{\bf\underbar{Stack}} &\qquad\hbox to 0 pt{\bf\underbar{Derivation}\hss}\cr\noalign{\medskip}
  \big((q_0,\varepsilon,\varepsilon),(q_1,S)\big)  &                      &\qquad S         &\cr
  \big((q_1,\varepsilon,S),(q_1,AB)\big)           &                      &\qquad AB        &\qquad S\ &\YIELDS AB\cr
  \big((q_1,\varepsilon,A),(q_1,aAb)\big)          &                      &\qquad aAbB      &          &\YIELDS aAbB\cr
  \big((q_1,a,a),(q_1,\varepsilon)\big)            &\qquad\qquad a        &\qquad AbB       &\cr
  \big((q_1,\varepsilon,A),(q_1,aAb)\big)          &\qquad\qquad a        &\qquad aAbbB     &          &\YIELDS aaAbbB\quad\cr
  \big((q_1,a,a),(q_1,\varepsilon)\big)            &\qquad\qquad aa       &\qquad AbbB      &&\cr
  \big((q_1,\varepsilon,A),(q_1,\varepsilon)\big)  &\qquad\qquad aa       &\qquad bbB       &          &\YIELDS aabbB\cr
  \big((q_1,b,b),(q_1,\varepsilon)\big)            &\qquad\qquad aab      &\qquad bB        &&\cr
  \big((q_1,b,b),(q_1,\varepsilon)\big)            &\qquad\qquad aabb     &\qquad B         &&\cr
  \big((q_1,\varepsilon,B),(q_1,bB)\big)           &\qquad\qquad aabb     &\qquad bB        &          &\YIELDS aabbbB\cr
  \big((q_1,b),(q_1,b,\varepsilon)\big)            &\qquad\qquad aabbb    &\qquad B         &&\cr
  \big((q_1,\varepsilon,B),(q_1,b)\big)            &\qquad\qquad aabbb    &\qquad b         &          &\YIELDS aabbbb\cr
  \big((q_1,b,b),(q_1,\varepsilon)\big)            &\qquad\qquad aabbbb   &\qquad           &&\cr
}

}


\bigskip


Note that at all times during this computation, the concatenation of the input that has been consumed
so far with the contents of the stack is equal to one of the strings in the left derivation.
Application of a rule of the form $\big((q_1,\sigma,\sigma),(q_1,\varepsilon)\big)$ has the
effect of removing one terminal symbol from the ``Stack'' column to the ``Input Consumed'' column.
Application of a rule of the form $\big((q_1,\varepsilon,A),(q_1,x)\big)$ has the
effect of applying the next step in the left derivation to the non-terminal symbol on the top
of the stack.  (In the ``Stack'' column, the pushdown automaton's stack is shown with its
top on the left.)  In the end, the entire input string has been consumed and the stack is empty, which
means that the string has been accepted by the pushdown automaton.  It should be easy to see
that for any context free grammar $G$, the same correspondence will always hold between
left derivations and computations performed by the pushdown automaton constructed from $G$.


\bigbreak

The computation of a pushdown automaton can involve nondeterminism.
That is, at some point in the computation, there might be more than
one transition rule that apply.  When this is not the case---that is,
when there is no circumstance in which two different transition rules
apply---then we say that the pushdown automaton is \nw[pushdown automaton!deterministic]{deterministic}.
Note that a deterministic pushdown automaton can have transition
rules of the form $\big((q_i,\varepsilon,x),(q_j,y)\big)$ (or
even $\big((q_i,\varepsilon,\varepsilon),(q_j,y)\big)$ if that is
the \emph{only} transition from state $q_i$).  Note also that is is possible
for a deterministic pushdown automaton to get ``stuck''; that is, it
is possible that no rules apply in some circumstances even though the
input has not been completely consumed or the stack is not empty.
If a deterministic pushdown automaton gets stuck while reading a string
$x$, then $x$ is not accepted by the automaton.

The automaton given at the beginning of this section,
which accepts the language $\{a^nb^n\st n\in\N\}$, 
is not deterministic.  However, it is easy to construct a deterministic
pushdown automaton for this language:

\medskip
\centerline{\scaledeps{2truein}{fig-5-pa-2}}
\smallskip

\noindent However, consider the language $\{ww^R\st w\in\{a,b\}^*\}$.  Here is
a pushdown automaton that accepts this language:

\medskip
\centerline{\scaledeps{2truein}{fig-5-pa-3}}
\smallskip

\noindent In state $q_0$, this machine copies the first part of its input
string onto the stack.  In state $q_1$, it tries to match the remainder of the
input against the contents of the stack.  In order for this to work, it must ``guess''
where the middle of the string occurs by following the transition
from state $q_0$ to state $q_1$.  
In this case, it is by no means clear that it is possible to
construct a deterministic pushdown automaton that accepts the same
language.

\medbreak

At this point, it might be tempting to define a deterministic context-free
language as one for which there exists a deterministic pushdown automaton
which accepts that language.  However, there is a technical problem with
this definition:  we need to make it possible for the pushdown automaton
to detect the end of the input string.  Consider the language
$\{w\st w\in\{a,b\}^* \AND n_a(w)=n_b(w)\}$, which consists of strings over the
alphabet $\{a,b\}$ in which the number of $a$'s is equal to the number
of $b$'s.  This language is accepted by the following pushdown automaton:

\medskip
\centerline{\scaledeps{3.3truein}{fig-5-pa-4}}
\smallskip

\noindent In this automaton, a $c$ is first pushed onto the stack,
and it remains on the bottom of the stack until the computation ends.
During the process of reading an input string,
if the machine is in state $q_3$, then the
number of $a$'s that have been read is greater than or equal to
the number of $b$'s that have been read, and the stack contains
(copies of) the excess $a$'s that have been read.  Similarly,
if the machine is in state $q_4$, then the
number of $b$'s that have been read is greater than or equal to
the number of $a$'s that have been read, and the stack contains
(copies of) the excess $b$'s that have been read.
As the computation proceeds, if the stack contains nothing but a $c$,
then the number of $a$'s that have been consumed by the machine
is equal to the number of $b$'s that have been consumed; in such
cases, the machine can pop the $c$ from the stack---leaving the
stack empty---and jump to state $q_2$.  If the entire string has
been read at that time, then the string is accepted.  This involves
nondeterminism because the automaton has to ``guess'' when to
jump to state $q_2$; it has no way of knowing whether it has
actually reached the end of the string.

Although this pushdown automaton is not deterministic, we can
modify it easily to get a deterministic pushdown automaton that
accepts a closely related language.  We just have to add a
special end-of-string symbol to the language.  We use the
symbol {\tt\$} for this purpose.  The following deterministic
automaton accepts the language $\{w\hbox{\tt\$}\st w\in \{a,b\}^*\AND n_a(w)=n_b(w)\}\,$:

\medskip
\centerline{\scaledeps{3.3truein}{fig-5-pa-4b}}
\smallskip

\noindent In this modified automaton, it is only possible for the
machine to reach the accepting state $q_2$ by reading the end-of-string
symbol at a time when the number of $a$'s that have been consumed is equal
to the number of $b$'s.  Taking our cue from this example, we define 
what it means for a language to be deterministic context-free as follows:

\begin{definition}
Let $L$ be a language over an alphabet $\Sigma$, and let {\tt\$} be
a symbol that is not in $\Sigma$.  We say that $L$ is a \nw[context-free language!deterministic]{deterministic
context-free language}
if there is a deterministic pushdown automaton
that accepts the language $L\hbox{\tt\$}$ (which is equal to 
$\{w\hbox{\tt\$}\st w\in L\}$).
\end{definition}

There are context-free languages that are not deterministic context-free.
This means that for pushdown automata, nondeterminism adds real power.
This contrasts with the case of finite automata, where deterministic
finite automata and nondeterministic finite automata are equivalent in
power in the sense that they accept the same class of languages.

A deterministic context-free language can be parsed efficiently.
LL(1) parsing and LR(1) parsing can both be defined in terms of deterministic
pushdown automata, although we have not pursued that approach here.







\begin{exercises}

\problem Identify the context-free language that is accepted by each of the following
pushdown automata.  Explain your answers.

\medskip

\ppart \vtop{\hbox{}\vskip-10pt\hbox{\scaledeps{2.5truein}{fig-5-pa-ex1}}}

\medskip

\ppart \vtop{\hbox{}\vskip-10pt\hbox{\scaledeps{3.5truein}{fig-5-pa-ex2}}}

\medskip

\ppart \vtop{\hbox{}\vskip-10pt\hbox{\scaledeps{3.5truein}{fig-5-pa-ex3}}}

\medskip

\ppart \vtop{\hbox{}\vskip-10pt\hbox{\scaledeps{2.25truein}{fig-5-pa-ex4}}}


\problem Let $B$ be the language over the alphabet $\{\,\hbox{\tt(}\,,\hbox{\tt)}\,\}$ that consists of 
strings of parentheses that are balanced in the sense that every left parenthesis has
a matching right parenthesis.  Examples include {\tt ()}, {\tt (())()}, {\tt((())())()(())},
and the empty string.  Find a deterministic pushdown automaton with a single state that
accepts the language $B$.  Explain how your automaton works, and explain the circumstances
in which it will \emph{fail} to accept a given string of parentheses.

\problem Suppose that $L$ is language over an alphabet $\Sigma$.
Suppose that there is a deterministic pushdown automaton that accepts $L$.
Show that $L$ is deterministic context-free.  That is, show how to construct
a deterministic pushdown automaton that accepts the language $L${\tt\$}.
(Assume that the symbol {\tt\$} is not in $\Sigma$.)

\problem Find a deterministic pushdown automaton that accepts the language $\{wcw^R\st w\in\{a,b\}^*\}$.

\problem Show that the language $\{a^nb^m\st n\not=m\}$ is deterministic context-free.

\problem Show that the language $L=\{w\in\{a,b\}^*\st n_a(w) > n_b(w)\}$ is deterministic context-free.

\problem Let $M=(Q,\Sigma,\Lambda,q_0,\partial,F)$ be a pushdown automaton.  Define $L^\prime(M)$ to be 
the language $L^\prime(M)=\{w\in\Sigma^*\ |$ it is possible for $M$ to start in state $q_0$,
read all of $w$, and end in an accepting state$\}$.  $L^\prime(M)$ differs from $L(M)$ in that
for $w\in L^\prime(M)$, we do not require that the stack be empty at the end of the computation.
\ppart Show that there is a pushdown automaton $M^\prime$ such that $L(M^\prime)=L^\prime(M)$.
\ppart Show that a language $L$ is context-free if and only if there is a pushdown automaton
$M$ such that $L=L^\prime(M)$.
\ppart Identify the language $L^\prime(M)$ for each of the automata in Exercise 1.

\problem Let $L$ be a regular language over an alphabet $\Sigma$, and let $K$
be a context-free language over the same alphabet.  Let $M=(Q,\Sigma,q_0,\delta,F)$ be a DFA that
accepts $L$, and let $N=(P,\Sigma,\Lambda,p_0,\partial,E))$ be a pushdown automaton that accepts $K$.
Show that the language $L\cap K$ is context-free by constructing a pushdown automaton
that accepts $L\cap K$.  The pushdown automaton can be constructed as a ``cross product''
of $M$ and $N$ in which the set of states is $Q\times P$.  The construction is analogous
to the proof that the intersection of two regular languages is regular, as outlined
in Exercise~3.6.7.


\end{exercises}




\section{Non-context-free Languages}\label{S-grammars-4}

We have seen that there are context-free languages that are not
regular.  The natural question arises, are there languages that
are not context-free?  It's easy to answer this question in the
abstract:  For a given alphabet $\Sigma$, there are uncountably
many languages over $\Sigma$, but there are only
countably many context-free languages over $\Sigma$.  It follows
that most languages are not context-free.  However, this answer
is not very satisfying since it doesn't give us any example of
a specific language that is not context-free.

As in the case of regular languages, one way to show that
a given language $L$ is not context-free is to find some property
that is shared by all context-free languages and then to show that
$L$ does not have that property.  For regular languages, the
Pumping Lemma gave us such a property.  It turns out that
there is a similar Pumping Lemma for context-free languages.
The proof of this lemma uses parse trees.  In the proof, we
will need a way of representing abstract parse trees, without
showing all the details of the tree.  The picture

\medskip
\centerline{\scaledeps{0.5truein}{fig-5-4}}
\smallskip

\noindent represents a parse tree which has the non-terminal symbol
$A$ at its root and the string $x$ along the ``bottom'' of the tree.
(That is, $x$ is the string made up of all the symbols at the
endpoints of the tree's branches, read from left to right.)  Note that
this could be a partial parse tree---something that could be a part of a
larger tree.  That is, we do not require $A$ to be the start symbol
of the grammar and we allow $x$ to contain both terminal and
non-terminal symbols.  The string $x$, which is along the bottom
of the tree, is referred to as the \nw[yield of a parse tree]{yield}
of the parse tree.  Sometimes, we need to show more explicit detail in
the tree.  For example, the picture

\medskip
\centerline{\scaledeps{1truein}{fig-5-5}}

\noindent represents a parse tree in which the yield is the
string $xyz$.  The string $y$ is the yield of a smaller tree, with
root $B$, which is contained within the larger tree.
Note that any of the strings $x$, $y$, or $z$ could be the
empty string.  

We will also need the concept of the \nw[height of
a parse tree]{height} of a parse tree.  The height of a parse tree is
the length of the longest path from the root of the tree to the
tip of one of its branches.

Like the version for regular languages, the Pumping Lemma for
context-free languages shows that any sufficiently long string
in a context-free language contains a pattern that can be repeated
to produce new strings that are also in the language.  However,
the pattern in this case is more complicated.  For regular
languages, the pattern arises because any sufficiently long path
through a given DFA must contain a loop.   For context-free
languages, the pattern arises because in a sufficiently
large parse tree, along a path from the root of the tree to the
tip of one of its branches, there must be some non-terminal
symbol that occurs more than once.

\begin{theorem}[Pumping Lemma for Context-free Languages]
Suppose that $L$ is a context-free language.
Then there is an integer $K$ such that any string $w\in L(G)$
with $|w|\ge K$ has the property that $w$ can be written
in the form $w=uxyzv$ where
\Item{$\bullet\,$}$x$ and $z$ are not both equal to the empty string;
\Item{$\bullet\,$}$|xyz|< K$; and
\Item{$\bullet\,$}For any $n\in\N$, the string $ux^nyz^nv$ is in $L$.
\end{theorem}
\begin{proof}
Let $G=(V,\Sigma,P,S)$ be a context-free grammar for the language $L$.
Let $N$ be the number of non-terminal symbols in $G$, plus 1.  That is,
$N=|V|+1$.  Consider all possible parse trees for the grammar $G$
with height less than or equal to $N$.  (Include parse trees with any
non-terminal symbol as root, not just parse trees with root $S$.) 
There are only finitely many such parse trees, and therefore there
are only finitely many different strings that are the yields of
such parse trees.  Let $K$ 
be an integer which is greater than the length of any such string.

Now suppose that $w$ is any string in $L$ whose length is greater
than or equal to $K$.  Then \textit{any} parse tree for $w$ must have height
greater than $N$.  (This follows since $|w|\ge K$ and the yield of 
any parse tree of height $\le N$ has length less than $K$.)
Consider a parse tree for $w$ of minimal size, that is one that contains
the smallest possible number of nodes.  Since the height of this parse
tree is greater than $N$, there is at least one path from the
root of the tree to tip of a branch of the tree that has length
greater than $N$.  Consider the longest such path.  The symbol at
the tip of this path is a terminal symbol, but all the other symbols
on the path are non-terminal symbols.  There are at least $N$ such
non-terminal symbols on the path.  Since the number of different
non-terminal symbols is $|V|$ and since $N=|V|+1$, some non-terminal
symbol must occur twice on the path.  In fact, some non-terminal
symbol must occur twice among the bottommost $N$ non-terminal
symbols on the path.  Call this symbol $A$.  Then we see that
the parse tree for $w$ has the form shown here:

\medskip
\centerline{\scaledeps{2truein}{fig-5-6}}

\noindent The structure of this tree breaks the string $w$ into
five substrings, as shown in the above diagram.
We then have $w=uxyzv$.  It only remains to show that $x$,
$y$, and $z$ satisfy the three requirements stated in the 
theorem.

Let $T$ refer to the entire parse tree, let $T_1$ refer to the 
parse tree whose root is the upper $A$ in the diagram, and 
let $T_2$ be the parse tree whose root is the lower $A$ in
the diagram.  Note that the height of $T_1$ is less than 
or equal to $N$.  (This follows from two facts:  The path shown in
$T_1$ from its root to its base has length less than or equal to
$N$, because we chose the two occurrences of $A$ to be among the $N$
bottommost non-terminal symbols along the path in $T$ from its
root to its base.  We know that there is no longer path from
the root of $T_1$ to its base, since we chose the path in $T$
to be the longest possible path from the root of $T$ to its base.)
Since any parse tree with height less than or equal to $N$ has
yield of length less than $K$, we see that $|xyz|<K$.

If we remove $T_1$ from $T$ and replace it with
a copy of $T_2$, the result is a parse tree with
yield $uyv$, so we see that the string $uyv$ is in the language
$L$.  
Now, suppose that both $x$ and $z$ are equal to the
empty string.  In that case, $w=uyv$, so the tree we have
created would be another parse tree for $w$.  But this tree
is smaller than $T$, so this would contradict the fact that
$T$ is the smallest parse tree for $w$.  We see that
$x$ and $z$ cannot both be the empty string.

If we remove $T_2$ from $T$ and replace it with a copy
of $T_1$, the result is a parse tree with yield $ux^2yz^2v$,
so we see that $ux^2yz^2v\in L$.  The two parse trees that
we have created look like this:

\medskip
\centerline{\scaledeps{4truein}{fig-5-7}}
\smallskip

\noindent Furthermore, we can apply the process of replacing
$T_2$ with a copy of $T_1$ to the tree on the right above to
create a parse tree with yield $ux^3yz^3v$.  Continuing in this
way, we see that $ux^nyz^nv\in L$ for all $n\in\N$.
This completes the proof of the theorem.
\end{proof}


Since this theorem guarantees that all context-free languages
have a certain property, it can be used to show that specific
languages are not context-free.  The method is to show that
the language in question does not have the property that is
guaranteed by the theorem.  We give two examples.

\begin{corrolary}\label{T-CFG-anbncn}
Let $L$ be the language $\{a^nb^nc^n \st n\in\N\}$.  Then
$L$ is not a context-free language.
\end{corrolary}
\begin{proof}
We give a proof by contradiction.  Suppose that $L$ is
context-free.  Then, by the Pumping Lemma for Context-free Languages,
there is an integer $K$ such that every string $w\in L$ with 
$|w|\ge K$ can be written in the form $w=uxyzv$ where
$x$ and $z$ are not both empty, $|xyz|<K$, and $ux^nyz^nv\in L$
for every $n\in\N$.

Consider the string $w=a^Kb^Kc^K$, which is in $L$,
and write $w=uxyzv$, where $u$, $x$, $y$, $z$, and $v$ satisfy the
stated conditions.  Since $|xyz|<K$, we see that if $xyz$
contains an $a$, then it cannot contain a $c$.  And if it
contains a $c$, then it cannot contain an $a$.  It is also possible
that $xyz$ is made up entirely of $b\text{'}s$.  In any of these cases,
the string $ux^2yz^2v$ cannot be in $L$, since it does not contain
equal numbers of $a\text{'}s$, $b\text{'}s$, and $c\text{'}s$.  But this contradicts
the fact that $ux^nyz^nv\in L$ for all $n\in\N$.  This contradiction
shows that the assumption that $L$ is context-free is incorrect.
\end{proof}
\smallskip

\begin{corrolary}
Let $\Sigma$ be any alphabet that contains at least two symbols.
Let $L$ be the language over $\Sigma$ defined by $L=\{ss\st s\in\Sigma^*\}$.
Then $L$ is not context-free.
\end{corrolary}
\begin{proof}
Suppose, for the sake of contradiction, that $L$ is
context-free.  Then, by the Pumping Lemma for Context-free Languages,
there is an integer $K$ such that every string $w\in L$ with 
$|w|\ge K$ can be written in the form $w=uxyzv$ where
$x$ and $z$ are not both empty, $|xyz|<K$, and $ux^nyz^nv\in L$
for every $n\in\N$.

Let $a$ and $b$ represent distinct symbols in $\Sigma$.
Let $s=a^Kba^Kb$ and let $w=ss=a^Kba^Kba^Kba^Kb$, which is in $L$.
Write $w=uxyzv$, where $u$, $x$, $y$, $z$, and $v$ satisfy the
stated conditions.

Since $|xyz|<K$, $x$ and $z$ can, together, contain no more than one $b$.  If
either $x$ or $y$ contains a $b$, then $ux^2yz^2v$ contains exactly five
$b\text{'}s$.  But any string in $L$ is of the form $rr$ for some string $r$
and so contains an even number of $b\text{'}s$.  The fact that
$ux^2yz^2z$ contains five $b\text{'}s$ contradicts the fact that $ux^2yz^2v\in L$.
So, we get a contradiction in the case where $x$ or $y$ contains a $b$.

Now, consider the case where $x$ and $y$ consist entirely of $a\text{'}s$.
Again since $|xyz|<K$, we must have either that $x$ and $y$ are both
contained in the same group of $a\text{'}s$ in the string $a^Kba^Kba^Kba^Kb$,
or that $x$ is contained in one group of $a\text{'}s$ and $y$ is contained in
the next.  In either case, it is easy to check that the string
$ux^2yz^2v$ is no longer of the form $rr$ for any string $r$,
which contradicts the fact that $ux^2yz^2v\in L$.

Since we are led to a contradiction in every case, we see that the
assumption that $L$ is context-free must be incorrect.
\end{proof}


Now that we have some examples of languages that are not context-free,
we can settle some other questions about context-free languages.
In particular, we can show that the intersection of two context-free
languages is not necessarily context-free and that the complement of
a context-free language is not necessarily context-free.

\begin{theorem}
The intersection of two context-free languages is not necessarily a
context-free language.
\end{theorem}
\begin{proof}
To prove this, it is only necessary to produce an example of two
context-free languages $L$ and $M$ such that $L\cap M$ is not a
context-free languages.  Consider the following languages, defined
over the alphabet $\Sigma=\{a,b,c\}$:
\begin{align*}
   L&=\{a^nb^nc^m \st n\in\N\text{ and }m\in\N\}\\
   M&=\{a^nb^mc^m \st n\in\N\text{ and }m\in\N\}
\end{align*}
Note that strings in $L$ have equal numbers of $a\text{'}s$ and $b\text{'}s$ while
strings in $M$ have equal numbers of $b\text{'}s$ and $c\text{'}s$.  It follows that
strings in $L\cap M$ have equal numbers of $a\text{'}s$, $b\text{'}s$, and $c\text{'}s$.
That is,
\begin{align*}
   L\cap M=\{a^nb^nc^n\st n\in\N\}
\end{align*}
We know from the above theorem that $L\cap M$ is not context-free.
However, both $L$ and $M$ are context-free.  The language $L$
is generated by the context-free grammar
\begin{align*}
   S &\PRODUCES TC\\
   C &\PRODUCES cC\\
   C &\PRODUCES \EMPTYSTRING\\
   T &\PRODUCES aTb\\
   T &\PRODUCES \EMPTYSTRING
\end{align*}
and $M$ is generated by a similar context-free grammar.
\end{proof}
\smallskip

\begin{corrolary}
The complement of a context-free language is not necessarily context-free.
\end{corrolary}
\begin{proof}
  Suppose for the sake of contradiction that the complement of every
context-free language is context-free.  

Let $L$ and $M$ be two context-free languages over the alphabet $\Sigma$.
By our assumption, the complements $\overline{L}$ and $\overline{M}$
are context-free.  By Theorem~\ref{T-CFG-closures}, it follows
that $\overline{L}\cup\overline{M}$ is context-free.  Applying our
assumption once again, we have that $\overline{\overline{L}\cup\overline{M}}$
is context-free.
But $\overline{\overline{L}\cup\overline{M}}=L\cap M$, so we have
that $L\cap M$ is context-free.

We have shown, based on our assumption that the complement of any
context-free language is context-free, that the intersection of any
two context-free languages is context-free.  But this contradicts
the previous theorem, so we see that the assumption cannot be true.
This proves the theorem.
\end{proof}

Note that the preceding theorem and corollary say only that
$L\cap M$ is not context-free for \textit{some} context-free languages
$L$ and $M$ and that $\overline{L}$ is not context-free for 
\textit{some} context-free language $L$.  There are, of course, many 
examples of context-free languages $L$ and $M$ for which
$L\cap M$ and $\overline{L}$ \textit{are} in fact context-free.

Even though the intersection of two context-free languages is not necessarily
context-free, it happens that the intersection of a context-free language
with a \textit{regular} language is always context-free.  This is not difficult
to show, and a proof is outlined in Exercise~4.4.8.
I state it here without proof:

\begin{theorem}
Suppose that $L$ is a context-free language and that $M$ is a regular
language.  Then $L\cap M$ is a context-free language.
\end{theorem}

For example, let $L$ and $M$ be the languages
defined by $L=\{w\in \{a,b\}^*\st w=w^R\}$ and
$M=\{w\in\{a,b\}^*\st$ the length of $w$ is a multiple of 5$\}$.  Since 
$L$ is context-free and $M$ is regular, we know that
$L\cap M$ is context-free.  The language $L\cap M$ consists of
every palindrome over the alphabet $\{a,b\}$ whose length is a
multiple of five.

This theorem can also be used to show that certain languages are
\textit{not} context-free.  For example, consider the language
$L=\{w\in\{a,b,c\}^*\st n_a(w)$ $=n_b(w)=n_c(w)\}$.  (Recall that
$n_x(w)$ is the number of times that the symbol $x$ occurs in the
string $w$.)  We can use a proof by contradiction to show that
$L$ is not context-free. 
Let $M$ be the regular language defined by the regular
expression $a^*b^*c^*$.  It is clear that
$L\cap M=\{a^nb^nc^n\st n\in\N\}$.  If $L$ were context-free,
then, by the previous theorem, $L\cap M$ would be context-free.
However, we know from Theorem~\ref{T-CFG-anbncn} that $L\cap M$
is not context-free.  So we can conclude that $L$ is not
context-free.


\begin{exercises}

\problem Show that the following languages are not context-free:
\ppart $\{a^nb^mc^k\st n>m>k\}$
\ppart $\{w\in\{a,b,c\}^*\st n_a(w)>n_b(w)>n_c(w)\}$
\ppart $\{www\st w\in\{a,b\}^*\}$
\ppart $\{a^nb^mc^k\st n,m\in\N\text{ and } k=m*n\}$
\ppart $\{a^nb^m\st m=n^2\}$

\problem Show that the languages $\{a^n\st \,\text{n is a prime number}\}$
and $\{a^{n^2}\st n\in \N\}$ are not context-free.  (In fact, it 
can be shown that a language over the alphabet $\{a\}$ is
context-free if and only if it is regular.)

\problem Show that the language $\{w\in\{a,b\}^*\st n_a(w)=n_b(w)$
and $w$ contains the string $baaab$ as a substring$\}$ is context-free.

\problem Suppose that $M$ is any finite language and that
$L$ is any context-free language.  Show that the language
$L\SETDIFF M$ is context-free.  (Hint: Any finite language is a
regular language.)

\end{exercises}



\section{General Grammars}\label{S-grammars-5}

At the beginning of this chapter the general idea of a grammar as a set of
rewriting or production rules was introduced.  For most of the chapter, however,
we have restricted our attention to context-free grammars, in which production
rules must be of the form $A\PRODUCES x$ where $A$ is a non-terminal symbol.
In this section, we will consider general grammars, that is, grammars in which
there is no such restriction on the form of production rules.  For a general
grammar, a production rule has the form $u\PRODUCES x$, where $u$ is string
that can contain both terminal and non-terminal symbols.  For convenience, we
will assume that $u$ contains at least one non-terminal symbol, although
even this restriction could be lifted without changing the class of languages
that can be generated by grammars.  Note that a context-free grammar is, in fact,
an example of a general grammar, since production rules in a general grammar
are allowed to be of the form $A\PRODUCES x$.  They just don't have to be of
this form.  I will use the unmodified term \nw{grammar}\index{general grammar} to
refer to general grammars.\footnote{There is another special type of grammar that
is intermediate between context-free grammars and general grammars.  In a
so-called \nw{context-sensitive grammar}, every production rule is of the form
$u\PRODUCES x$ where $|x|\ge|u|$.  We will not cover context-sensitive grammars in
this text.}  The definition of grammar is identical to the
definition of context-free grammar, except for the form of the production rules:

\begin{definition}
A \nw{grammar} is a 4-tuple $(V,\Sigma,P,S)$,
where:

\Item{1.\ }$V$ is a finite set of symbols.  The elements of $V$
are the non-terminal symbols of the grammar.

\Item{2.\ }$\Sigma$ is a finite set of symbols such that $V\cap\Sigma=\emptyset$.
The elements of $\Sigma$ are the terminal symbols of the grammar.

\Item{3.\ }$P$ is a set of production rules.  Each rule is of the
form $u\PRODUCES x$ where $u$ and $x$ are strings in $(V\cup \Sigma)^*$
and $u$ contains at least one symbol from $V$.

\Item{4.\ }$S\in V$.  $S$ is the start symbol of the grammar.
\end{definition}

Suppose $G$ is a grammar.  Just as in the context-free case,
the language generated by $G$ is denoted by $L(G)$ and is defined
as $L(G)=\{x\in\Sigma^*\st S \YIELDS_G^* x\}$.  That is, a string
$x$ is in $L(G)$ if and only if $x$ is a string of terminal symbols
and there is a derivation that produces $x$ from the start symbol,
$S$, in one or more steps.

The natural question is whether there are languages that can be generated
by general grammars but that cannot be generated by context-free languages.
We can answer this question immediately by giving an example of such
a language.  Let $L$ be the language $L=\{w\in\{a,b,c\}^*\st n_a(w)=n_b(w)=n_c(w)\}$.
We saw at the end of the last section that $L$ is not context-free.
However, $L$ is generated by the following grammar:
\begin{align*}
  S&\PRODUCES SABC\\
  S&\PRODUCES \EMPTYSTRING\\
  AB&\PRODUCES BA\\
  BA&\PRODUCES AB\\
  AC&\PRODUCES CA\\
  CA&\PRODUCES AC\\
  BC&\PRODUCES CB\\
  CB&\PRODUCES BC\\
  A&\PRODUCES a\\
  B&\PRODUCES b\\
  C&\PRODUCES c
\end{align*}
For this grammar, the set of non-terminals is $\{S,A,B,C\}$ and the set of
terminal symbols is $\{a,b,c\}$.  Since both terminals and non-terminal
symbols can occur on the left-hand side of a production rule in a general
grammar, it is not possible, in general, to determine which symbols are non-terminal and
which are terminal just by looking at the list of production rules.
However, I will follow the convention that uppercase letters are always
non-terminal symbols.  With this convention, I can continue to specify a
grammar simply by listing production rules.

The first two rules in the above grammar make it possible to produce the strings
$\EMPTYSTRING$, $ABC$, $ABCABC$, $ABCABCABC$, and so on.  Each of these strings
contains equal numbers of $A\text{'}s$, $B\text{'}s$, and $C\text{'}s$.
The next six rules allow the order of the non-terminal symbols in the string
to be changed.  They make it possible to arrange the $A\text{'}s$, $B\text{'}s$,
and $C\text{'}s$ into any arbitrary order.  Note that these rules could not occur
in a context-free grammar.  The last three rules convert the non-terminal symbols
$A$, $B$, and $C$ into the corresponding terminal symbols $a$, $b$, and $c$.
Remember that all the non-terminals must be eliminated in order to produce
a string in $L(G)$.  Here, for example, is a derivation of the string
$baabcc$ using this grammar.  In each line, the string that will be replaced
on the next line is underlined.
\begin{align*}
   S&\YIELDS \underline{S}ABC\\
    &\YIELDS \underline{S}ABCABC\\
    &\YIELDS \underline{AB}CABC\\
    &\YIELDS BA\underline{CA}BC\\
    &\YIELDS BAA\underline{CB}C\\
    &\YIELDS \underline{B}AABCC\\
    &\YIELDS b\underline{A}ABCC\\
    &\YIELDS ba\underline{A}BCC\\
    &\YIELDS baa\underline{B}CC\\
    &\YIELDS baab\underline{C}C\\
    &\YIELDS baabc\underline{C}\\
    &\YIELDS baabcc
\end{align*}
We could produce any string in $L$ in a similar way.  Of course,
this only shows that $L\SUB L(G)$.  To show that $L(G)\SUB L$,
we can observe that for any string $w$ such that $S\;\YIELDSTAR w$,
$n_A(w)+n_a(w) = n_B(w)+n_b(w) = n_C(w)+n_c(w)$.  This follows since the
rule $S\YIELDS SABC$ produces strings in which $n_A(w)=n_B(w)=n_C(w)$,
and no other rule changes any of the quantities
$n_A(w)+n_a(w)$, $n_B(w)+n_b(w)$, or $n_C(w)+n_c(w)$.
After applying these rules to produce a string $x\in L(G)$, we must
have that $n_A(x)$, $n_B(x)$, and $n_C(x)$ are zero.  The fact that
$n_a(x)=n_b(x)=n_c(x)$ then follows from the fact that
$n_A(x)+n_a(x) = n_B(x)+n_b(x) = n_C(x)+n_c(x)$.  That is, $x\in L$.
\medskip

Our first example of a non-context-free language was $\{a^nb^nc^n\st n\in \N\}$.
This language can be generated by a general grammar similar to the previous
example.  However, it requires some cleverness to force the $a\text{'}s$,
$b\text{'}s$, and $c\text{'}s$ into the correct order.  To do this, instead of
allowing $A\text{'}s$, $B\text{'}s$, and $C\text{'}s$ to transform themselves
spontaneously into $a\text{'}s$, $b\text{'}s$, and $c\text{'}s$, we use additional
non-terminal symbols to transform them only after they are in the correct position.
Here is a grammar that does this:
\begin{align*}
  S&\PRODUCES SABC\\
  S&\PRODUCES X\\
  BA&\PRODUCES AB\\
  CA&\PRODUCES AC\\
  CB&\PRODUCES BC\\
  XA&\PRODUCES aX\\
  X&\PRODUCES Y\\
  YB&\PRODUCES bY\\
  Y&\PRODUCES Z\\
  ZC&\PRODUCES cZ\\
  Z&\PRODUCES \EMPTYSTRING
\end{align*}
Here, the first two rules produce one of the strings $X$, $XABC$, $XABCABC$,
$XABCABCABC$, and so on.  The next three rules allow $A\text{'s}$ to move to the
left and $C\text{'}s$ to move to the right, producing a string of the form $XA^nB^nC^n$,
for some $n\in\N$.  The rule $XA\PRODUCES aX$ allows the
$X$ to move through the $A\text{'}s$ from left to right, converting $A\text{'}s$
to $a\text{'}s$ as it goes.  After converting the $A\text{'}s$, the $X$ can be
transformed into a $Y$.  The $Y$ will then move through the $B\text{'}s$, converting
them to $b\text{'}s$.  Then, the $Y$ is transformed into a $Z$, which is responsible
for converting $C\text{'}s$ to $c\text{'}s$.  Finally, an application of the
rule $Z\PRODUCES\EMPTYSTRING$ removes the $Z$, leaving the string $a^nb^nc^n$.

Note that if the rule $X\PRODUCES Y$ is applied before all the $A\text{'}s$ have
been converted to $a\text{'}s$, then there is no way for the remaining $A\text{'}s$
to be converted to $a\text{'}s$ or otherwise removed from the string.  This means
that the derivation has entered a dead end, which can never produce a string
that consists of terminal symbols only.  The only derivations that can produce
strings in the language generated by the grammar are derivations in which the
$X$ moves past all the $A\text{'}s$, converting them all to $a\text{'}s$.  At this
point in the derivation, the string is of the form $a^nXu$ where $u$ is a string
consisting entirely of $B\text{'}s$ and $C\text{'}s$.  At this point, the
rule $X\PRODUCES Y$ can be applied, producing the string $a^nYu$.  Then, if a string
of terminal symbols is ever to be produced, the $Y$ must move past all the $B\text{'}s$,
producing the string $a^nb^nYC^n$.  You can see that the use of three separate
non-terminals, $X$, $Y$, and $Z$, is essential for forcing the symbols in
$a^nb^nc^n$ into the correct order.

\medbreak

For one more example, consider the language $\{a^{n^2}\st n\in\N\}$.  Like the other
languages we have considered in this section, this language is not context-free.
However, it can be generated by a grammar.  Consider the grammar
\begin{align*}
  S&\PRODUCES DTE\\
  T&\PRODUCES BTA\\
  T&\PRODUCES \EMPTYSTRING\\
  BA&\PRODUCES AaB\\
  Ba&\PRODUCES aB\\
  BE&\PRODUCES E\\
  DA&\PRODUCES D\\
  Da&\PRODUCES aD\\
  DE&\PRODUCES \EMPTYSTRING
\end{align*}
The first three rules produce all strings of the form $DB^nA^nE$, for $n\in\N$.
Let's consider what happens to the string $DB^nA^nE$ as the remaining rules are applied.
The next two rules allow a $B$ to move to the right until it reaches the $E$.
Each time the $B$ passes an $A$, a new $a$ is generated, but a $B$ will simply
move past an $a$ without generating any other characters.  Once the $B$ reaches
the $E$, the rule $BE\PRODUCES E$ makes the $B$ disappear.  Each $B$ from the
string $DB^nA^nE$ moves past $n$ $A\text{'}s$ and generates $n$ $a\text{'}s$.
Since there are $n$ $B\text{'}s$, a total of $n^2$ $a\text{'}s$ are generated.
Now, the only way to get rid of the $D$ at the beginning of the string is for
it to move right through all the $A\text{'}s$ and $a\text{'}s$ until it reaches
the $E$ at the end of the string.  As it does this, the rule $DA\PRODUCES D$
eliminates all the $A\text{'}s$ from the string, leaving the string $a^{n^2}DE$.
Applying the rule $DE\PRODUCES\EMPTYSTRING$ to this gives $a^{n^2}$.  This
string contains no non-terminal symbols and so is in the language generated
by the grammar.  We see that every string of the form $a^{n^2}$ is generated
by the above grammar.  Furthermore, only strings of this form can be generated
by the grammar.  

\medbreak

Given a fixed alphabet $\Sigma$, there are only countably many different
languages over $\Sigma$ that can be generated by grammars.  Since there
are uncountably many different languages over $\Sigma$, we know that
there are many languages that cannot be generated by grammars.
However, it is surprisingly difficult to find an actual example of
such a language.

As a first guess, you might suspect that just as $\{a^nb^n \st n\in\N\}$
is an example of a language that is not regular and
$\{a^nb^nc^n\st n\in\N\}$ is an example of a language that is not
context-free, so $\{a^nb^nc^nd^n\st n\in\N\}$ might be an example
of a language that cannot be generated by any grammar.  However,
this is not the case.  The same technique that was used
to produce a grammar that generates $\{a^nb^nc^n\st n\in\N\}$ can
also be used to produce a grammar for $\{a^nb^nc^nd^n\st n\in\N\}$.
In fact, the technique extends to similar languages based on any
number of symbols.

Or you might guess that there is no grammar for the
language $\{a^n\st\,$ $n$ is a prime number$\,\}$.  Certainly, producing
prime numbers doesn't seem like the kind of thing that we would
ordinarily do with a grammar.  Nevertheless, there is a grammar that
generates this language.  We will not actually write down the grammar,
but we will eventually have a way to prove that it exists.

The language $\{a^{n^2}\st n\in\N\}$ really doesn't seem all that
``grammatical'' either, but we produced a grammar for it above.
If you think about how this grammar works, you might get the feeling
that its operation is more like ``computation'' than ``grammar.''
This is our clue.  A grammar can be thought of as a kind of program,
albeit one that is executed in a nondeterministic fashion.  It turns
out that general grammars are precisely as powerful as any other
general-purpose programming language, such as Java or C++.  More
exactly, a language can be generated by a grammar if and only if
there is a computer program whose output consists of a list 
containing all the strings and only the
strings in that language.  Languages that have this property
are said to be \nw[recursively enumerable language]{recursively 
enumerable languages}.  (This term
as used here is {\it not\/} closely related to the idea of a recursive
subroutine.)  The languages that can be generated by general
grammars are precisely the recursively enumerable languages.
We will return to this topic in the next chapter.

It turns out that there are many forms of computation that are
precisely equivalent in power to grammars and to computer programs,
and no one has ever found any form of computation that is more
powerful.  This is one of the great discoveries of the twentieth
century, and we will investigate it further in the next chapter.


\begin{exercises}

\problem Find a derivation for the string $caabcb$, according to the first example
grammar in this section.
Find a derivation for the string $aabbcc$, according to the second example
grammar in this section.
Find a derivation for the string $aaaa$, according to the third example
grammar in this section.

\problem Consider the third sample grammar from this section, which generates
the language $\{a^{n^2}\st n\in\N\}$.  Is the non-terminal symbol $D$ necessary
in this grammar?  What if the first rule of the grammar were replaced by
$S\PRODUCES TE$ and the last three rules were replaced by $A\PRODUCES\EMPTYSTRING$
and $E\PRODUCES\EMPTYSTRING\,$?  Would the resulting grammar still generate
the same language?  Why or why not?

\problem Find a grammar that generates the language $L=\{w\in\{a,b,c,d\}^*\st
n_a(w)=n_b(w)=n_c(w)=n_d(w)\}$.  Let $\Sigma$ be any alphabet.
Argue that the language $\{w\in\Sigma^*\st\,$ all symbols in $\Sigma$ occur equally
often in $w\,\}$ can be generated by a grammar.

\problem For each of the following languages, find a grammar that generates
the language.  In each case, explain how your grammar works.
\pparts{
   \{a^nb^nc^nd^n\st n\in\N\}&
   \{a^nb^mc^{nm}\st n\in\N\text{ and }m\in\N\}\cr
   \{ww\st w\in\{a,b\}^*\}&
   \{www\st w\in\{a,b\}^*\}\cr
   \{a^{2^n}\st n\in\N\}&
   \{w\in\{a,b,c\}^*\st n_a(w)>n_b(w)>n_c(w)\}\cr
}


\end{exercises}




\endinput

